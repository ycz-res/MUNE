"""
è®­ç»ƒæ¨¡å—
åŒ…å«è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¨¡å‹ä¿å­˜ç­‰åŠŸèƒ½
"""

from torch.utils.data import DataLoader
import torch
import torch.optim as optim
import torch.nn as nn
import argparse
import matplotlib.pyplot as plt
import os
from datetime import datetime
import pandas as pd
import numpy as np

from dataset import SimDataset
from config import get_config
from model import LinearModel
from loss import MuThresholdLoss
from utils import set_seed


def get_args_parser():
    a_parser = argparse.ArgumentParser('VLP scripts', add_help=False)
    a_parser.add_argument('--batch_size', default=1, type=int)
    a_parser.add_argument('--epochs', default=20, type=int)
    a_parser.add_argument('--shuffle', default=True, type=bool)
    a_parser.add_argument('--num_workers', default=0, type=int)
    a_parser.add_argument('--pin_memory', default=False, type=bool)
    a_parser.add_argument('--device', default='cpu', type=str)

    return a_parser

def main(args):
    # è®¾ç½®éšæœºç§å­
    set_seed(57)
    
    config = get_config()
    
    # åˆ›å»ºè®­ç»ƒæ›²çº¿ä¿å­˜æ–‡ä»¶å¤¹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    curves_dir = f"training_curves_{timestamp}"
    os.makedirs(curves_dir, exist_ok=True)
    print(f"è®­ç»ƒæ›²çº¿å°†ä¿å­˜åˆ°: {curves_dir}")
    
    # åˆå§‹åŒ–è®­ç»ƒå†å²è®°å½•
    train_losses = []
    val_losses = []
    epochs = []
    
    print("=== åŠ è½½æ•°æ®é›†===")
    # ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®æ–‡ä»¶çš„ä¸åŒç™¾åˆ†æ¯”èŒƒå›´ï¼Œå¹¶æŒ‡å®šé˜¶æ®µæ ‡ç­¾
    train_dataset = SimDataset(config['SimDataset.data'], 'sim', start_percent=0.0, end_percent=0.05, stage='train')  # å‰70%
    val_dataset = SimDataset(config['SimDataset.data'], 'sim', start_percent=0.05, end_percent=0.1, stage='val')  # 70%-85%
    test_dataset = SimDataset(config['SimDataset.data'], 'sim', start_percent=0.1, end_percent=0.15, stage='test')  # 85%-100%
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                             collate_fn=SimDataset.collate_fn, num_workers=args.num_workers, 
                             pin_memory=args.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, 
                           collate_fn=SimDataset.collate_fn, num_workers=args.num_workers, 
                           pin_memory=args.pin_memory)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                            collate_fn=SimDataset.collate_fn, num_workers=args.num_workers, 
                            pin_memory=args.pin_memory)

    
    model = LinearModel(d_model=64, max_thresholds=160)
    model.to(args.device)

    optimizer = optim.AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.95), weight_decay=1e-3)
    
    # ä½¿ç”¨MUæ•°é‡å’Œé˜ˆå€¼çš„åŠ æƒæŸå¤±å‡½æ•°
    criterion = MuThresholdLoss
    
    # è®­ç»ƒæ¨¡å‹
    print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {args.epochs} ä¸ªepoch")
    
    for epoch in range(args.epochs):
        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, args.device, epoch, args.epochs)
        
        # éªŒè¯ä¸€ä¸ªepoch
        val_loss = validate_one_epoch(model, val_loader, criterion, args.device, epoch, args.epochs)
        
        # è®°å½•è®­ç»ƒå†å²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        epochs.append(epoch + 1)
        
        print("-" * 50)
    
    print("è®­ç»ƒå®Œæˆï¼")
    
    # æµ‹è¯•
    print("\n=== æµ‹è¯•é˜¶æ®µ ===")
    avg_test_loss = test_one_epoch(model, test_loader, criterion, args.device)
    print(f"æµ‹è¯•å®Œæˆ, å¹³å‡æŸå¤±: {avg_test_loss:.6f}")
    
    # ç”Ÿæˆç»¼åˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    generate_comprehensive_training_visualization(epochs, train_losses, val_losses, curves_dir, args)
    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {curves_dir}")


def train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, total_epochs):
    model.train()
    total_loss = 0.0
    batch_count = 0
    
    for batch_idx, batch in enumerate(train_loader):
        src, tgt = batch
        src = src.to(device)
        tgt = tgt.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        nus, thresholds = model(src)
        
        # è®¡ç®—æŸå¤±
        # è¿™é‡Œéœ€è¦æ ¹æ®tgtçš„æ ¼å¼æ¥è®¡ç®—æŸå¤±
        # tgtæ ¼å¼: [nus, threshold1, threshold2, ...]
        nus_target = tgt[:, 0:1]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºMUæ•°é‡ç›®æ ‡
        thresholds_target = tgt[:, 1:]  # å–å‰©ä½™å…ƒç´ ä½œä¸ºé˜ˆå€¼ç›®æ ‡
        loss = criterion(nus, nus_target, thresholds, thresholds_target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        if batch_idx % 10 == 0:
            print(f"Epoch {epoch+1}/{total_epochs}, Batch {batch_idx}, Loss: {loss.item():.6f}")
    
    avg_loss = total_loss / batch_count
    print(f"Epoch {epoch+1}/{total_epochs} è®­ç»ƒå®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.6f}")
    return avg_loss


def validate_one_epoch(model, val_loader, criterion, device, epoch, total_epochs):
    model.eval()
    val_loss = 0.0
    val_batch_count = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            src, tgt = batch
            src = src.to(device)
            tgt = tgt.to(device)
            
            nus, thresholds = model(src)
            nus_target = tgt[:, 0:1]
            thresholds_target = tgt[:, 1:]
            loss = criterion(nus, nus_target, thresholds, thresholds_target)
            
            val_loss += loss.item()
            val_batch_count += 1
    
    avg_val_loss = val_loss / val_batch_count
    print(f"Epoch {epoch+1}/{total_epochs} éªŒè¯å®Œæˆ, å¹³å‡æŸå¤±: {avg_val_loss:.6f}")
    return avg_val_loss


def test_one_epoch(model, test_loader, criterion, device):
    model.eval()
    test_loss = 0.0
    test_batch_count = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            src, tgt = batch
            src = src.to(device)
            tgt = tgt.to(device)
            
            nus, thresholds = model(src)
            nus_target = tgt[:, 0:1]
            thresholds_target = tgt[:, 1:]
            loss = criterion(nus, nus_target, thresholds, thresholds_target)
            
            test_loss += loss.item()
            test_batch_count += 1
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªé¢„æµ‹ç»“æœ
            if batch_idx < 3:
                print(f"æµ‹è¯•æ‰¹æ¬¡ {batch_idx+1}:")
                print(f"  çœŸå®MUæ•°é‡: {nus_target.squeeze().tolist()}")
                print(f"  é¢„æµ‹MUæ•°é‡: {nus.squeeze().tolist()}")
                print(f"  æŸå¤±: {loss.item():.6f}")
    
    avg_test_loss = test_loss / test_batch_count
    return avg_test_loss


def generate_comprehensive_training_visualization(epochs, train_losses, val_losses, save_dir, args):
    """
    ç”Ÿæˆç»¼åˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    
    Args:
        epochs: epochåˆ—è¡¨
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
        args: è®­ç»ƒå‚æ•°
    """
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå¤§å‹ç»¼åˆå›¾è¡¨
    fig = plt.figure(figsize=(20, 16))
    
    # 1. ä¸»è®­ç»ƒæ›²çº¿ (å·¦ä¸Š)
    ax1 = plt.subplot(3, 3, 1)
    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2.5, marker='o', markersize=3)
    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2.5, marker='s', markersize=3)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training and Validation Loss Curves', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)
    ax1.set_facecolor('#f8f9fa')
    
    # 2. æŸå¤±è¶‹åŠ¿åˆ†æ (ä¸­ä¸Š)
    ax2 = plt.subplot(3, 3, 2)
    train_diff = [train_losses[i] - train_losses[i-1] for i in range(1, len(train_losses))]
    val_diff = [val_losses[i] - val_losses[i-1] for i in range(1, len(val_losses))]
    ax2.plot(epochs[1:], train_diff, 'b-', label='Training Loss Change', linewidth=2)
    ax2.plot(epochs[1:], val_diff, 'r-', label='Validation Loss Change', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Loss Change', fontsize=12)
    ax2.set_title('Loss Change Trend Analysis', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)
    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax2.set_facecolor('#f8f9fa')
    
    # 3. è®­ç»ƒç¨³å®šæ€§åˆ†æ (å³ä¸Š)
    ax3 = plt.subplot(3, 3, 3)
    train_ma = [np.mean(train_losses[max(0, i-4):i+1]) for i in range(len(train_losses))]
    val_ma = [np.mean(val_losses[max(0, i-4):i+1]) for i in range(len(val_losses))]
    ax3.plot(epochs, train_ma, 'b-', label='Training MA(5)', linewidth=2)
    ax3.plot(epochs, val_ma, 'r-', label='Validation MA(5)', linewidth=2)
    ax3.set_xlabel('Epoch', fontsize=12)
    ax3.set_ylabel('Moving Average Loss', fontsize=12)
    ax3.set_title('Training Stability (5-Epoch Moving Average)', fontsize=14, fontweight='bold')
    ax3.legend(fontsize=11)
    ax3.grid(True, alpha=0.3)
    ax3.set_facecolor('#f8f9fa')
    
    # 4. æœ€ç»ˆæŸå¤±å¯¹æ¯” (å·¦ä¸­)
    ax4 = plt.subplot(3, 3, 4)
    final_train_loss = train_losses[-1]
    final_val_loss = val_losses[-1]
    bars = ax4.bar(['Training', 'Validation'], [final_train_loss, final_val_loss], 
                   color=['#3498db', '#e74c3c'], alpha=0.8, edgecolor='black', linewidth=1)
    ax4.set_ylabel('Final Loss', fontsize=12)
    ax4.set_title('Final Loss Comparison', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')
    ax4.set_facecolor('#f8f9fa')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, [final_train_loss, final_val_loss]):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 5. è¿‡æ‹Ÿåˆåˆ†æ (ä¸­ä¸­)
    ax5 = plt.subplot(3, 3, 5)
    overfitting_gap = [val_losses[i] - train_losses[i] for i in range(len(epochs))]
    ax5.plot(epochs, overfitting_gap, 'g-', linewidth=2.5, marker='o', markersize=3)
    ax5.set_xlabel('Epoch', fontsize=12)
    ax5.set_ylabel('Overfitting Gap', fontsize=12)
    ax5.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3)
    ax5.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax5.set_facecolor('#f8f9fa')
    
    # 6. æŸå¤±åˆ†å¸ƒç›´æ–¹å›¾ (å³ä¸­)
    ax6 = plt.subplot(3, 3, 6)
    ax6.hist(train_losses, bins=15, alpha=0.7, color='blue', label='Training', edgecolor='black')
    ax6.hist(val_losses, bins=15, alpha=0.7, color='red', label='Validation', edgecolor='black')
    ax6.set_xlabel('Loss Value', fontsize=12)
    ax6.set_ylabel('Frequency', fontsize=12)
    ax6.set_title('Loss Distribution', fontsize=14, fontweight='bold')
    ax6.legend(fontsize=11)
    ax6.grid(True, alpha=0.3)
    ax6.set_facecolor('#f8f9fa')
    
    # 7. è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ (å·¦ä¸‹)
    ax7 = plt.subplot(3, 3, 7)
    ax7.axis('off')
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    best_train_loss = min(train_losses)
    best_val_loss = min(val_losses)
    train_range = max(train_losses) - min(train_losses)
    val_range = max(val_losses) - min(val_losses)
    final_overfitting = final_val_loss - final_train_loss
    
    stats_text = f"""
    ğŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    
    ğŸ¯ è®­ç»ƒé…ç½®:
    â€¢ æ€»Epochæ•°: {len(epochs)}
    â€¢ æ‰¹æ¬¡å¤§å°: {args.batch_size}
    â€¢ å­¦ä¹ ç‡: 1e-5
    â€¢ ä¼˜åŒ–å™¨: AdamW
    
    ğŸ“ˆ æŸå¤±ç»Ÿè®¡:
    â€¢ æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.6f}
    â€¢ æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.6f}
    â€¢ æœ€ä½³è®­ç»ƒæŸå¤±: {best_train_loss:.6f}
    â€¢ æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}
    
    ğŸ“Š å˜åŒ–èŒƒå›´:
    â€¢ è®­ç»ƒæŸå¤±èŒƒå›´: {train_range:.6f}
    â€¢ éªŒè¯æŸå¤±èŒƒå›´: {val_range:.6f}
    â€¢ æœ€ç»ˆè¿‡æ‹Ÿåˆç¨‹åº¦: {final_overfitting:.6f}
    
    ğŸ¯ è®­ç»ƒè´¨é‡è¯„ä¼°:
    â€¢ æ”¶æ•›æ€§: {'è‰¯å¥½' if train_range < 1.0 else 'éœ€æ”¹è¿›'}
    â€¢ è¿‡æ‹Ÿåˆ: {'è½»å¾®' if abs(final_overfitting) < 0.5 else 'æ˜æ˜¾'}
           â€¢ ç¨³å®šæ€§: {'ç¨³å®š' if len(train_diff) > 0 and max(abs(x) for x in train_diff[-5:]) < 0.1 else 'æ³¢åŠ¨'}
    """
    
    ax7.text(0.05, 0.95, stats_text, transform=ax7.transAxes, 
             fontsize=10, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='#ecf0f1', alpha=0.8))
    
    # 8. è®­ç»ƒé˜¶æ®µåˆ†æ (ä¸­ä¸‹)
    ax8 = plt.subplot(3, 3, 8)
    # å°†è®­ç»ƒåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µ
    total_epochs = len(epochs)
    early_stage = total_epochs // 3
    mid_stage = 2 * total_epochs // 3
    
    early_train = np.mean(train_losses[:early_stage])
    mid_train = np.mean(train_losses[early_stage:mid_stage])
    late_train = np.mean(train_losses[mid_stage:])
    
    early_val = np.mean(val_losses[:early_stage])
    mid_val = np.mean(val_losses[early_stage:mid_stage])
    late_val = np.mean(val_losses[mid_stage:])
    
    stages = ['Early\n(1-33%)', 'Middle\n(34-66%)', 'Late\n(67-100%)']
    train_means = [early_train, mid_train, late_train]
    val_means = [early_val, mid_val, late_val]
    
    x = np.arange(len(stages))
    width = 0.35
    
    ax8.bar(x - width/2, train_means, width, label='Training', color='#3498db', alpha=0.8)
    ax8.bar(x + width/2, val_means, width, label='Validation', color='#e74c3c', alpha=0.8)
    
    ax8.set_xlabel('Training Stage', fontsize=12)
    ax8.set_ylabel('Average Loss', fontsize=12)
    ax8.set_title('Loss by Training Stage', fontsize=14, fontweight='bold')
    ax8.set_xticks(x)
    ax8.set_xticklabels(stages)
    ax8.legend(fontsize=11)
    ax8.grid(True, alpha=0.3, axis='y')
    ax8.set_facecolor('#f8f9fa')
    
    # 9. è®­ç»ƒå»ºè®® (å³ä¸‹)
    ax9 = plt.subplot(3, 3, 9)
    ax9.axis('off')
    
    # åŸºäºåˆ†æç»™å‡ºå»ºè®®
    recommendations = []
    if final_overfitting > 1.0:
        recommendations.append("ğŸ”§ å»ºè®®å¢åŠ æ­£åˆ™åŒ–æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    if train_range > 2.0:
        recommendations.append("ğŸ“‰ å»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ‰¹æ¬¡å¤§å°")
    if len(train_diff) > 0 and max(abs(x) for x in train_diff[-5:]) > 0.2:
        recommendations.append("âš¡ å»ºè®®ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    if best_val_loss > best_train_loss * 1.5:
        recommendations.append("ğŸ¯ å»ºè®®å¢åŠ è®­ç»ƒæ•°æ®æˆ–æ•°æ®å¢å¼º")
    
    if not recommendations:
        recommendations.append("âœ… è®­ç»ƒæ•ˆæœè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­å½“å‰é…ç½®")
    
    rec_text = "ğŸ’¡ è®­ç»ƒå»ºè®®:\n\n" + "\n".join(recommendations)
    
    ax9.text(0.05, 0.95, rec_text, transform=ax9.transAxes, 
             fontsize=11, verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round,pad=0.5', facecolor='#d5f4e6', alpha=0.8))
    
    # è®¾ç½®æ•´ä½“æ ‡é¢˜
    fig.suptitle('MUNEæ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ç»¼åˆåˆ†ææŠ¥å‘Š', fontsize=18, fontweight='bold', y=0.98)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    plt.subplots_adjust(top=0.94, hspace=0.3, wspace=0.3)
    
    # ä¿å­˜ç»¼åˆå›¾è¡¨
    plt.savefig(os.path.join(save_dir, 'comprehensive_training_analysis.png'), 
                dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # ä¿å­˜è®­ç»ƒæ•°æ®åˆ°CSV
    import pandas as pd
    training_data = pd.DataFrame({
        'Epoch': epochs,
        'Training_Loss': train_losses,
        'Validation_Loss': val_losses,
        'Loss_Difference': [val_losses[i] - train_losses[i] for i in range(len(epochs))],
        'Training_Loss_Change': [0] + train_diff,
        'Validation_Loss_Change': [0] + val_diff
    })
    training_data.to_csv(os.path.join(save_dir, 'detailed_training_data.csv'), index=False)
    
    print(f"ğŸ“Š ç»¼åˆè®­ç»ƒåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'comprehensive_training_analysis.png')}")
    print(f"ğŸ“ˆ è¯¦ç»†è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {os.path.join(save_dir, 'detailed_training_data.csv')}")


if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)

