"""
è®­ç»ƒæ¨¡å—
åŒ…å«è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¨¡å‹ä¿å­˜ç­‰åŠŸèƒ½
"""

from torch.utils.data import DataLoader
import torch
import torch.optim as optim
import argparse
import os
from datetime import datetime
import numpy as np
import time

from dataset import Sim
from config import get_config
from model import Linear, CNN, LSTM
from loss import ce, focal_ce, thr
from visualization import MUThresholdVisualizer
from utils import set_seed
from metrics import b_v_metrics


def get_args_parser():
    a_parser = argparse.ArgumentParser('MU Threshold Prediction Training', add_help=False)
    a_parser.add_argument('--batch_size', default=4, type=int, help='Batch size for training')
    a_parser.add_argument('--epochs', default=10, type=int, help='Number of training epochs')
    a_parser.add_argument('--shuffle', default=True, type=bool, help='Shuffle training data')
    a_parser.add_argument('--num_workers', default=0, type=int, help='Number of data loading workers')
    a_parser.add_argument('--pin_memory', default=False, type=bool, help='Pin memory for data loading')
    a_parser.add_argument('--device', default='cpu', type=str, help='Device to use (cpu/cuda)')
    a_parser.add_argument('--lr', default=1e-4, type=float, help='Learning rate')
    a_parser.add_argument('--weight_decay', default=1e-3, type=float, help='Weight decay')
    a_parser.add_argument('--patience', default=5, type=int, help='Early stopping patience')
    a_parser.add_argument('--loss_type', default='ce', choices=['thr', 'focal', 'ce'], help='Loss function type: thr=threshold loss, focal=focal loss, ce=cross entropy')
    a_parser.add_argument('--model_type', default='LSTM', choices=['Linear', 'CNN', 'LSTM'], help='Model architecture type')
    a_parser.add_argument('--save_best', default=True, type=bool, help='Save best model')
    a_parser.add_argument('--save_dir', default='checkpoints', type=str, help='Directory to save models')
    a_parser.add_argument('--threshold_mode', default='binary', choices=['value', 'binary'], help='Threshold output mode: binary=0/1 mask, value=actual threshold values')
    a_parser.add_argument('--dataset_type', default='Sim', choices=['Sim'], help='Dataset type')
    a_parser.add_argument('--metrics_threshold', default=0.5, type=float, help='Threshold for metrics calculation (0.1-0.3 recommended for sparse data)')
    a_parser.add_argument('--use_weighted_loss', default=False, type=bool, help='Use weighted loss for imbalanced data')
    a_parser.add_argument('--pos_weight', default=50.0, type=float, help='Positive class weight for weighted loss')
    
    return a_parser

def main(args):
    # è®¾ç½®éšæœºç§å­
    set_seed(57)
    
    config = get_config()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    curves_dir = f"plot/training_curves_{timestamp}"
    os.makedirs(curves_dir, exist_ok=True)
    os.makedirs(args.save_dir, exist_ok=True)
    
    Dataset = eval(args.dataset_type)
    # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼šè®­ç»ƒé›†90%ï¼ŒéªŒè¯é›†5%ï¼Œæµ‹è¯•é›†5%
    train_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.0, end_percent=0.9, stage='train', threshold_mode=args.threshold_mode)
    val_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.9, end_percent=0.95, stage='val', threshold_mode=args.threshold_mode)
    test_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.95, end_percent=1.0, stage='test', threshold_mode=args.threshold_mode)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                             collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                             pin_memory=args.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, 
                           collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                           pin_memory=args.pin_memory)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                            collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                            pin_memory=args.pin_memory)

    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    model = eval(args.model_type)(d_model=64).to(args.device)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=args.weight_decay)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆæ”¯æŒåŠ æƒï¼‰
    if args.use_weighted_loss and args.loss_type == 'ce':
        pos_weight_tensor = torch.tensor(args.pos_weight, device=args.device)
        def loss_fn(pred, target):
            return ce(pred, target, pos_weight=pos_weight_tensor)
    else:
        loss_fn = eval(args.loss_type)
    
    # åˆ›å»ºæŒ‡æ ‡å‡½æ•°ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼ï¼‰
    def metrics_fn(pred, target):
        return b_v_metrics(pred, target, threshold=args.metrics_threshold)
    
    # è®­ç»ƒçŠ¶æ€
    best_score = float('inf')  # æŸå¤±è¶Šå°è¶Šå¥½
    best_epoch = 0
    patience_counter = 0
    training_history = []  # å­˜å‚¨è®­ç»ƒå†å²
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {args.model_type} + {args.loss_type} | æ•°æ®é›†: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)} | Epochs: {args.epochs}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ”„ Epoch {epoch+1}/{args.epochs} å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒå’ŒéªŒè¯
        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, args.device, epoch+1, args.epochs)
        val_loss, val_metrics = validate_epoch(model, val_loader, loss_fn, metrics_fn, args.device)
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°åŸºç¡€æŒ‡æ ‡
        print(f"â±ï¸  Epoch {epoch+1} å®Œæˆï¼Œè€—æ—¶: {epoch_time:.2f}ç§’")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        if val_metrics:
            print(f"ğŸ“ˆ éªŒè¯æŒ‡æ ‡: {val_metrics}")
        else:
            print("ğŸ“ˆ éªŒè¯æŒ‡æ ‡: None")
        
        # è®°å½•è®­ç»ƒå†å²
        training_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_metrics': val_metrics
        })
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜ï¼ˆä½¿ç”¨éªŒè¯æŸå¤±æŒ‡å¯¼ï¼‰
        current_loss = val_loss
        is_best = current_loss < best_score  # æŸå¤±è¶Šå°è¶Šå¥½
        
        if is_best:
            best_score = current_loss
            best_epoch = epoch + 1
            patience_counter = 0
            save_model(model, optimizer, epoch + 1, best_score, val_metrics, args.save_dir, timestamp)
            print(f"ğŸ¯ æ–°æœ€ä½³æ¨¡å‹! Val_Loss={best_score:.4f} â­ (è€å¿ƒå€¼é‡ç½®)")
        else:
            patience_counter += 1
            print(f"â³ è€å¿ƒå€¼: {patience_counter}/{args.patience} (Val_Loss={current_loss:.4f})")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"â¹ï¸ æ—©åœè§¦å‘! è¿ç»­ {args.patience} ä¸ªepochæ— æ”¹å–„")
            break
    
    print(f"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} ä¸ªepochï¼ŒéªŒè¯æŸå¤±: {best_score:.4f}")
    
    # æµ‹è¯•é˜¶æ®µ
    load_best_model(model, args.save_dir, timestamp)
    print("ğŸ§ª æµ‹è¯•é˜¶æ®µ")
    test_loss, test_metrics = validate_epoch(model, test_loader, loss_fn, metrics_fn, args.device)
    
    # æ”¶é›†éšæœºæµ‹è¯•æ ·æœ¬ç”¨äºå¯è§†åŒ–
    print("ğŸ“Š æ”¶é›†æµ‹è¯•æ ·æœ¬ç”¨äºå¯è§†åŒ–...")
    sample_data = collect_test_samples(model, test_loader, args.device, num_samples=20, threshold=args.metrics_threshold)
    
    # æ‰“å°æµ‹è¯•æŒ‡æ ‡
    print(f"âœ… æµ‹è¯•å®Œæˆ, å¹³å‡æŸå¤±: {test_loss:.6f}")
    if test_metrics:
        print(f"test_metrics: {test_metrics}")
    else:
        print("test_metrics: None")
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    generate_training_report(
        training_history=training_history,
        test_loss=test_loss,
        test_metrics=test_metrics,
        sample_data=sample_data,
        save_dir=curves_dir
    )


def train_epoch(model, train_loader, optimizer, loss_fn, device, current_epoch, total_epochs):
    model.train()
    total_loss = 0.0
    batch_count = 0
    total_batches = len(train_loader)
    
    # æ¯10ä¸ªbatchæˆ–æ¯25%è¿›åº¦æ‰“å°ä¸€æ¬¡
    print_interval = max(1, total_batches // 10)  # è‡³å°‘æ¯10%æ‰“å°ä¸€æ¬¡
    if total_batches < 10:
        print_interval = max(1, total_batches // 4)  # å°æ•°æ®é›†æ—¶æ›´é¢‘ç¹
    
    batch_start_time = time.time()
    
    for batch_idx, batch in enumerate(train_loader):
        src, tgt = batch
        # ç§»åŠ¨åˆ°è®¾å¤‡
        src = {key: value.to(device) for key, value in src.items()}
        tgt = {key: value.to(device) for key, value in tgt.items()}
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
        
        # è®¡ç®—æŸå¤±
        loss = loss_fn(thresholds_pred, tgt["thresholds"])
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:
            progress = (batch_idx + 1) / total_batches * 100
            batch_time = time.time() - batch_start_time
            avg_batch_time = batch_time / print_interval
            current_avg_loss = total_loss / batch_count
            
            print(f"  ğŸ“¦ Batch {batch_idx+1}/{total_batches} ({progress:.1f}%) | "
                  f"Loss: {current_avg_loss:.4f} | "
                  f"é€Ÿåº¦: {avg_batch_time:.2f}s/batch")
            
            batch_start_time = time.time()
        
    avg_loss = total_loss / batch_count
    return avg_loss


def validate_epoch(model, val_loader, loss_fn, metrics_fn, device):
    model.eval()
    val_loss = 0.0
    val_batch_count = 0
    total_val_batches = len(val_loader)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡
    all_predictions = []
    all_targets = []
    
    print(f"  ğŸ” å¼€å§‹éªŒè¯ ({total_val_batches} batches)...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            src, tgt = batch
            # ç§»åŠ¨åˆ°è®¾å¤‡
            src = {key: value.to(device) for key, value in src.items()}
            tgt = {key: value.to(device) for key, value in tgt.items()}
            
            thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
            loss = loss_fn(thresholds_pred, tgt["thresholds"])
            
            val_loss += loss.item()
            val_batch_count += 1
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            all_predictions.append(thresholds_pred)
            all_targets.append(tgt["thresholds"])
            
            # éªŒè¯è¿›åº¦æç¤ºï¼ˆåªåœ¨éªŒè¯é›†è¾ƒå¤§æ—¶æ˜¾ç¤ºï¼‰
            if total_val_batches > 5 and (batch_idx + 1) % max(1, total_val_batches // 5) == 0:
                progress = (batch_idx + 1) / total_val_batches * 100
                print(f"    ğŸ” éªŒè¯è¿›åº¦: {batch_idx+1}/{total_val_batches} ({progress:.0f}%)")
    
    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    if all_predictions:
        all_pred = torch.cat(all_predictions, dim=0)
        all_true = torch.cat(all_targets, dim=0)
        
        # ä½¿ç”¨ç»¼åˆè¯„ä»·æŒ‡æ ‡
        val_metrics = metrics_fn(all_pred, all_true)
        
        avg_val_loss = val_loss / val_batch_count
        print(f"  âœ… éªŒè¯å®Œæˆ: {val_batch_count} batches")
        return avg_val_loss, val_metrics
    else:
        raise RuntimeError("éªŒè¯é˜¶æ®µæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æŒ‡æ ‡å‡½æ•°")


def generate_training_report(training_history, test_loss, test_metrics, sample_data, save_dir):
    """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨"""
    visualizer = MUThresholdVisualizer(save_dir)
    
    # æ›´æ–°è®­ç»ƒå†å²
    for epoch_data in training_history:
        visualizer.update_epoch(
            epoch_data['epoch'], 
            epoch_data['train_loss'], 
            epoch_data['val_loss'],
            metrics=epoch_data.get('val_metrics')
        )
    
    # è®¾ç½®æµ‹è¯•ç»“æœ
    if test_metrics:
        visualizer.set_test_results(test_loss, test_metrics)
    
    # è®¾ç½®æ ·æœ¬æ•°æ®
    if sample_data:
        visualizer.set_sample_data(
            sample_data['indices'],
            sample_data['cmap'],
            sample_data['thresholds_true'],
            sample_data['thresholds_pred'],
            sample_data['mus_true']
        )
    
    # ç”Ÿæˆå››å¼ å›¾
    visualizer.generate_four_figs()
    
    print(f"ğŸ“Š è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {save_dir}")


def save_model(model, optimizer, epoch, best_score, val_metrics, save_dir, timestamp):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    model_path = os.path.join(save_dir, f'best_model_{timestamp}.pth')
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_score': best_score,
        'val_metrics': val_metrics
    }, model_path)


def collect_test_samples(model, test_loader, device, num_samples=20, threshold=0.5):
    """ç›´æ¥é€šè¿‡ç´¢å¼•æ”¶é›†éšæœºæµ‹è¯•æ ·æœ¬"""
    model.eval()
    
    print(f"  ğŸ” æ”¶é›† {num_samples} ä¸ªéšæœºæµ‹è¯•æ ·æœ¬...")
    
    # è·å–æµ‹è¯•é›†æ€»æ ·æœ¬æ•°
    test_dataset = test_loader.dataset
    total_samples = len(test_dataset)
    
    # è°ƒæ•´æ ·æœ¬æ•°
    if num_samples > total_samples:
        num_samples = total_samples
        print(f"  âš ï¸  è¯·æ±‚æ ·æœ¬æ•°è¶…è¿‡æµ‹è¯•é›†å¤§å°ï¼Œè°ƒæ•´ä¸º {num_samples}")
    
    # ç”Ÿæˆéšæœºç´¢å¼•
    random_indices = torch.randperm(total_samples)[:num_samples].tolist()
    print(f"  ğŸ“Š ä» {total_samples} ä¸ªæ ·æœ¬ä¸­éšæœºé€‰æ‹©äº† {num_samples} ä¸ª")
    
    # ç›´æ¥é€šè¿‡ç´¢å¼•è·å–æ ·æœ¬
    cmap_list = []
    thresholds_true_list = []
    mus_true_list = []
    
    for idx in random_indices:
        cmap_data, mu_count, threshold_data = test_dataset[idx]
        cmap_list.append(cmap_data)
        thresholds_true_list.append(threshold_data)
        mus_true_list.append(mu_count)
    
    # è½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°è®¾å¤‡
    cmap_tensor = torch.stack(cmap_list).to(device)
    
    # æ‰¹é‡é¢„æµ‹
    print(f"  ğŸ¯ å¯¹ {num_samples} ä¸ªæ ·æœ¬è¿›è¡Œæ‰¹é‡é¢„æµ‹ï¼ˆé˜ˆå€¼={threshold}ï¼‰...")
    with torch.no_grad():
        thresholds_pred_raw = model(cmap_tensor)
        # äºŒå€¼åŒ–é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨æŒ‡å®šé˜ˆå€¼ï¼‰
        thresholds_pred = (torch.sigmoid(thresholds_pred_raw) >= threshold).float()
    
    # ç»„è£…ç»“æœ
    sample_data = {
        'indices': random_indices,
        'cmap': cmap_tensor.cpu().numpy(),
        'thresholds_true': torch.stack(thresholds_true_list).cpu().numpy(),
        'thresholds_pred': thresholds_pred.cpu().numpy(),
        'mus_true': torch.stack(mus_true_list).cpu().numpy()
    }
    
    print(f"  âœ… æ ·æœ¬æ”¶é›†å®Œæˆ: {len(sample_data['indices'])} ä¸ªæ ·æœ¬")
    return sample_data


def load_best_model(model, save_dir, timestamp):
    """åŠ è½½æœ€ä½³æ¨¡å‹"""
    model_path = os.path.join(save_dir, f'best_model_{timestamp}.pth')
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path)
        model.load_state_dict(checkpoint['model_state_dict'])



if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)

