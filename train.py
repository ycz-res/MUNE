"""
è®­ç»ƒæ¨¡å—
åŒ…å«è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¨¡å‹ä¿å­˜ç­‰åŠŸèƒ½
"""

from torch.utils.data import DataLoader
import torch
import torch.optim as optim
import argparse
import os
from datetime import datetime
import numpy as np
import time
import warnings

# å¿½ç•¥ NVML è­¦å‘Š
warnings.filterwarnings('ignore', message='.*NVML.*')

from dataset import Sim
from config import get_config
from model import Linear, CNN, LSTM, MUNECNN, Transformer
from loss import ce, focal_ce, thr, emd
from utils import set_seed
from metrics import b_v_metrics
from visualization import plot_single_sample
import json


def get_args_parser():
    a_parser = argparse.ArgumentParser('MU Threshold Prediction Training', add_help=False)
    a_parser.add_argument('--batch_size', default=32, type=int, help='Batch size for training')
    a_parser.add_argument('--epochs', default=10, type=int, help='Number of training epochs')
    a_parser.add_argument('--num_workers', default=8, type=int, help='Number of data loading workers')
    a_parser.add_argument('--pin_memory', default=True, type=bool, help='Pin memory for data loading')
    a_parser.add_argument('--device', default='cuda', type=str, help='Device to use (cpu/cuda)')
    a_parser.add_argument('--lr', default=1e-4, type=float, help='Learning rate')
    a_parser.add_argument('--weight_decay', default=1e-3, type=float, help='Weight decay (L2 regularization)')
    a_parser.add_argument('--grad_clip', default=1.0, type=float, help='Gradient clipping value (0=disabled)')
    a_parser.add_argument('--patience', default=20, type=int, help='Early stopping patience')
    a_parser.add_argument('--loss_type', default='emd', choices=['thr', 'focal', 'ce', 'emd'], help='Loss function type: thr=threshold loss, focal=focal loss, ce=cross entropy, emd=earth mover\'s distance')
    a_parser.add_argument('--model_type', default='LSTM', choices=['Linear', 'CNN', 'LSTM', 'MUNECNN', 'Transformer'], help='Model architecture type')
    a_parser.add_argument('--save_best', default=True, type=bool, help='Save best model')
    a_parser.add_argument('--result_dir', default='result', type=str, help='Root directory to save experiment results')
    a_parser.add_argument('--timestamp', default=None, type=str, help='Experiment timestamp (e.g., 20251023_123456). If not provided, auto-generate')
    a_parser.add_argument('--threshold_mode', default='binary', choices=['value', 'binary'], help='Threshold output mode: binary=0/1 mask, value=actual threshold values')
    a_parser.add_argument('--dataset_type', default='Sim', choices=['Sim', 'Real'], help='Dataset type')
    a_parser.add_argument('--metrics_threshold', default=0.5, type=float, help='Threshold for metrics calculation (0.5 is standard, consistent with test)')
    a_parser.add_argument('--use_weighted_loss', default=True, type=bool, help='Use weighted loss for imbalanced data (only works with --loss_type ce)')
    a_parser.add_argument('--pos_weight', default=5.0, type=float, help='Positive class weight for CE loss only (ignored for other loss types)')
    a_parser.add_argument('--d_model', default=128, type=int, help='Model hidden dimension (default: 128, larger for better capacity)')
    a_parser.add_argument('--lr_scheduler', default='cosine', choices=['none', 'cosine', 'plateau'], help='Learning rate scheduler type')
    a_parser.add_argument('--warmup_epochs', default=5, type=int, help='Warmup epochs for cosine scheduler')
    a_parser.add_argument('--dropout', default=0.1, type=float, help='Dropout rate for regularization (0.0-1.0)')
    
    return a_parser

def main(args):
    # è®¾ç½®éšæœºç§å­
    set_seed(57)
    
    config = get_config()
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›äº†timestampåˆ™ä½¿ç”¨ï¼Œå¦åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
    if args.timestamp:
        timestamp = args.timestamp
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = os.path.join(args.result_dir, timestamp)
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(os.path.join(result_dir, "checkpoints"), exist_ok=True)
    
    Dataset = eval(args.dataset_type)
    # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼šè®­ç»ƒé›†90%ï¼ŒéªŒè¯é›†5%ï¼Œæµ‹è¯•é›†5%
    train_dataset = Dataset(config['SimDataset.data'], args.dataset_type, start_percent=0.0, end_percent=0.9, stage='train', threshold_mode=args.threshold_mode)
    val_dataset = Dataset(config['SimDataset.data'], args.dataset_type, start_percent=0.9, end_percent=0.95, stage='val', threshold_mode=args.threshold_mode)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                             collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                             pin_memory=args.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, 
                           collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                           pin_memory=args.pin_memory)


    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    model = eval(args.model_type)(d_model=args.d_model, dropout=args.dropout).to(args.device)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=args.weight_decay)
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.lr_scheduler == 'cosine':
        from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
        warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=args.warmup_epochs)
        cosine_scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs - args.warmup_epochs, eta_min=args.lr * 0.01)
        scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[args.warmup_epochs])
        print(f"ğŸ“ˆ ä½¿ç”¨Cosineå­¦ä¹ ç‡è°ƒåº¦å™¨ (Warmup={args.warmup_epochs} epochs)")
    elif args.lr_scheduler == 'plateau':
        from torch.optim.lr_scheduler import ReduceLROnPlateau
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
        print(f"ğŸ“ˆ ä½¿ç”¨Plateauå­¦ä¹ ç‡è°ƒåº¦å™¨ (patience=5)")
    else:
        scheduler = None
        print(f"ğŸ“ˆ ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    # æ³¨æ„ï¼špos_weight åªåœ¨ CE æŸå¤±æ—¶ç”Ÿæ•ˆ
    if args.use_weighted_loss and args.loss_type == 'ce':
        pos_weight_tensor = torch.tensor(args.pos_weight, device=args.device)
        def loss_fn(pred, target):
            return ce(pred, target, pos_weight=pos_weight_tensor)
        print(f"ğŸ“Š ä½¿ç”¨åŠ æƒCEæŸå¤±ï¼Œæ­£æ ·æœ¬æƒé‡: {args.pos_weight}")
    else:
        loss_fn = eval(args.loss_type)
    
    # åˆ›å»ºæŒ‡æ ‡å‡½æ•°ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼å’Œæ¨¡å¼ï¼‰
    def metrics_fn(pred, target):
        return b_v_metrics(pred, target, mode=args.threshold_mode, threshold=args.metrics_threshold)
    
    # è®­ç»ƒçŠ¶æ€
    best_score = float('inf')  # æŸå¤±è¶Šå°è¶Šå¥½
    best_epoch = 0
    patience_counter = 0
    training_history = []  # å­˜å‚¨è®­ç»ƒå†å²
    
    # ç»„è£…ä¿å­˜è·¯å¾„ï¼ˆä¿å­˜åˆ°result/{timestamp}/ç›®å½•ï¼‰
    best_model_path = os.path.join(result_dir, "checkpoints", f'best_model_{timestamp}.pth')
    train_data_path = os.path.join(result_dir, f'train_{timestamp}.json')
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {args.model_type} + {args.loss_type} | æ•°æ®é›†: {len(train_dataset)}/{len(val_dataset)} | Epochs: {args.epochs}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ”„ Epoch {epoch+1}/{args.epochs} å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒå’ŒéªŒè¯
        visual_dir = os.path.join(result_dir, 'train_visual') if result_dir else None
        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, args.device, epoch+1, args.grad_clip, visual_dir)
        val_loss, val_metrics = validate_epoch(model, val_loader, loss_fn, metrics_fn, args.device)
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°åŸºç¡€æŒ‡æ ‡
        print(f"â±ï¸  Epoch {epoch+1} å®Œæˆï¼Œè€—æ—¶: {epoch_time:.2f}ç§’")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        if val_metrics:
            print(f"ğŸ“ˆ éªŒè¯æŒ‡æ ‡: {val_metrics}")
        else:
            print("ğŸ“ˆ éªŒè¯æŒ‡æ ‡: None")
        
        # è®°å½•è®­ç»ƒå†å²
        training_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_metrics': val_metrics
        })
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜ï¼ˆä½¿ç”¨éªŒè¯æŸå¤±æŒ‡å¯¼ï¼‰
        current_loss = val_loss
        is_best = current_loss < best_score  # æŸå¤±è¶Šå°è¶Šå¥½
        
        if is_best:
            best_score = current_loss
            best_epoch = epoch + 1
            patience_counter = 0
            save_model(model, optimizer, epoch + 1, best_score, val_metrics, best_model_path)
            print(f"ğŸ¯ æ–°æœ€ä½³æ¨¡å‹! Val_Loss={best_score:.4f} â­ (è€å¿ƒå€¼é‡ç½®)")
        else:
            patience_counter += 1
            print(f"â³ è€å¿ƒå€¼: {patience_counter}/{args.patience} (Val_Loss={current_loss:.4f})")
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler is not None:
            if args.lr_scheduler == 'plateau':
                scheduler.step(current_loss)
            else:
                scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            print(f"ğŸ“‰ å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"â¹ï¸ æ—©åœè§¦å‘! è¿ç»­ {args.patience} ä¸ªepochæ— æ”¹å–„")
            break
    
    print(f"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} ä¸ªepochï¼ŒéªŒè¯æŸå¤±: {best_score:.4f}")
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    print("\nğŸ“Š ä¿å­˜è®­ç»ƒæ•°æ®...")
    save_training_data(
        training_history=training_history,
        save_path=train_data_path,
        timestamp=timestamp,
        best_model_path=best_model_path,
        args=args,
        config=config
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
    print(f"   - æ¨¡å‹è·¯å¾„: {best_model_path}")
    print(f"   - è®­ç»ƒæ•°æ®: {train_data_path}")
    print(f"\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæµ‹è¯•:")
    print(f"   python3 test.py --checkpoint {best_model_path}")


def train_epoch(model, train_loader, optimizer, loss_fn, device, current_epoch, grad_clip=1.0, visual_dir=None):
    model.train()
    total_loss = 0.0
    batch_count = 0
    total_batches = len(train_loader)
    
    # æ¯10ä¸ªbatchæˆ–æ¯25%è¿›åº¦æ‰“å°ä¸€æ¬¡
    print_interval = max(1, total_batches // 10)  # è‡³å°‘æ¯10%æ‰“å°ä¸€æ¬¡
    if total_batches < 10:
        print_interval = max(1, total_batches // 4)  # å°æ•°æ®é›†æ—¶æ›´é¢‘ç¹
    
    batch_start_time = time.time()
    
    for batch_idx, batch in enumerate(train_loader):
        src, tgt = batch
        # ç§»åŠ¨åˆ°è®¾å¤‡
        src = {key: value.to(device) for key, value in src.items()}
        tgt = {key: value.to(device) for key, value in tgt.items()}
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
        
        # æ¯100ä¸ªbatchç”»ä¸€æ¬¡å›¾
        if visual_dir and batch_idx % 100 == 0:
            os.makedirs(visual_dir, exist_ok=True)
            try:
                save_path = os.path.join(visual_dir, f'train_{current_epoch}_{batch_idx}.png')
                plot_single_sample(src, thresholds_pred, tgt["thresholds"], save_path, epoch=current_epoch)
            except Exception as e:
                print(f"âš ï¸  è®­ç»ƒæ ·æœ¬å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        
        # è®¡ç®—æŸå¤±
        loss = loss_fn(thresholds_pred, tgt["thresholds"])
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        if grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:
            progress = (batch_idx + 1) / total_batches * 100
            batch_time = time.time() - batch_start_time
            avg_batch_time = batch_time / print_interval
            current_avg_loss = total_loss / batch_count
            
            print(f"  ğŸ“¦ Batch {batch_idx+1}/{total_batches} ({progress:.1f}%) | "
                  f"Loss: {current_avg_loss:.4f} | "
                  f"é€Ÿåº¦: {avg_batch_time:.2f}s/batch")
            
            batch_start_time = time.time()
        
    avg_loss = total_loss / batch_count
    return avg_loss


def validate_epoch(model, val_loader, loss_fn, metrics_fn, device):
    model.eval()
    val_loss = 0.0
    val_batch_count = 0
    total_val_batches = len(val_loader)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡
    all_predictions = []
    all_targets = []
    
    print(f"  ğŸ” å¼€å§‹éªŒè¯ ({total_val_batches} batches)...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            src, tgt = batch
            # ç§»åŠ¨åˆ°è®¾å¤‡
            src = {key: value.to(device) for key, value in src.items()}
            tgt = {key: value.to(device) for key, value in tgt.items()}
            
            thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
            loss = loss_fn(thresholds_pred, tgt["thresholds"])
            
            val_loss += loss.item()
            val_batch_count += 1
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            all_predictions.append(thresholds_pred)
            all_targets.append(tgt["thresholds"])
            
            # éªŒè¯è¿›åº¦æç¤ºï¼ˆåªåœ¨éªŒè¯é›†è¾ƒå¤§æ—¶æ˜¾ç¤ºï¼‰
            if total_val_batches > 5 and (batch_idx + 1) % max(1, total_val_batches // 5) == 0:
                progress = (batch_idx + 1) / total_val_batches * 100
                print(f"    ğŸ” éªŒè¯è¿›åº¦: {batch_idx+1}/{total_val_batches} ({progress:.0f}%)")
    
    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    if all_predictions:
        all_pred = torch.cat(all_predictions, dim=0)
        all_true = torch.cat(all_targets, dim=0)
        
        # ä½¿ç”¨ç»¼åˆè¯„ä»·æŒ‡æ ‡
        val_metrics = metrics_fn(all_pred, all_true)
        
        avg_val_loss = val_loss / val_batch_count
        print(f"  âœ… éªŒè¯å®Œæˆ: {val_batch_count} batches")
        return avg_val_loss, val_metrics
    else:
        raise RuntimeError("éªŒè¯é˜¶æ®µæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æŒ‡æ ‡å‡½æ•°")


def save_training_data(training_history, save_path, timestamp, best_model_path, args=None, config=None):
    """ä¿å­˜è®­ç»ƒæ•°æ®ä¸ºJSONæ ¼å¼"""
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    training_data = {
        'timestamp': timestamp,
        'total_epochs': len(training_history),
        'best_model_path': best_model_path,
        'training_history': training_history
    }
    
    # æ·»åŠ argsé…ç½®ï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
    if args is not None:
        training_data['config_args'] = vars(args)
    
    # æ·»åŠ configé…ç½®ï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
    if config is not None:
        training_data['config'] = config.to_dict() if hasattr(config, 'to_dict') else config
    
    # ä¿å­˜ä¸ºJSON
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {save_path}")


def save_model(model, optimizer, epoch, best_score, val_metrics, save_path):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_score': best_score,
        'val_metrics': val_metrics
    }, save_path)





if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)

