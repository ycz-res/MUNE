"""
è®­ç»ƒæ¨¡å—
åŒ…å«è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¨¡å‹ä¿å­˜ç­‰åŠŸèƒ½
"""

from torch.utils.data import DataLoader
import torch
import torch.optim as optim
import argparse
import os
from datetime import datetime
import numpy as np
import time
import warnings

# å¿½ç•¥ NVML è­¦å‘Š
warnings.filterwarnings('ignore', message='.*NVML.*')

from dataset import Sim
from config import get_config
from model import Linear, CNN, LSTM, MUNECNN, Transformer
import loss
from utils import set_seed
from metrics import b_v_metrics
from visualize import plot_single_sample
import json
import sys


def get_args_parser():
    a_parser = argparse.ArgumentParser('MU Threshold Prediction Training', add_help=False)
    a_parser.add_argument('--batch_size', default=32, type=int, help='Batch size for training')
    a_parser.add_argument('--epochs', default=10, type=int, help='Number of training epochs')
    a_parser.add_argument('--num_workers', default=8, type=int, help='Number of data loading workers')
    a_parser.add_argument('--pin_memory', default=True, type=bool, help='Pin memory for data loading')
    a_parser.add_argument('--device', default='cuda', type=str, help='Device to use (cpu/cuda)')
    a_parser.add_argument('--lr', default=1e-4, type=float, help='Learning rate')
    a_parser.add_argument('--weight_decay', default=1e-3, type=float, help='Weight decay (L2 regularization)')
    a_parser.add_argument('--grad_clip', default=1.0, type=float, help='Gradient clipping value (0=disabled)')
    a_parser.add_argument('--patience', default=20, type=int, help='Early stopping patience')
    a_parser.add_argument('--loss_type', default='emd', 
                         choices=['ce', 'weighted_bce', 'dice', 'iou', 'f1', 'count', 'emd', 'hamming',
                                 'jaccard', 'tversky', 'focal_tversky', 'combo', 'mixed'],
                         help='Loss function type. Available: ce, weighted_bce, dice, iou, f1, count, emd, hamming, jaccard, tversky, focal_tversky, combo, mixed (mixed uses LOSS_CONFIG from loss.py)')
    a_parser.add_argument('--model_type', default='LSTM', choices=['Linear', 'CNN', 'LSTM', 'MUNECNN', 'Transformer'], help='Model architecture type')
    a_parser.add_argument('--save_best', default=True, type=bool, help='Save best model')
    a_parser.add_argument('--result_dir', default='result', type=str, help='Root directory to save experiment results')
    a_parser.add_argument('--timestamp', default=None, type=str, help='Experiment timestamp (e.g., 20251023_123456). If not provided, auto-generate')
    a_parser.add_argument('--threshold_mode', default='binary', choices=['value', 'binary'], help='Threshold output mode: binary=0/1 mask, value=actual threshold values')
    a_parser.add_argument('--dataset_type', default='Sim', choices=['Sim', 'Real'], help='Dataset type')
    a_parser.add_argument('--metrics_threshold', default=0.5, type=float, help='Threshold for metrics calculation (0.5 is standard, consistent with test)')
    a_parser.add_argument('--d_model', default=128, type=int, help='Model hidden dimension (default: 128, larger for better capacity)')
    a_parser.add_argument('--lr_scheduler', default='cosine', choices=['none', 'cosine', 'plateau'], help='Learning rate scheduler type')
    a_parser.add_argument('--warmup_epochs', default=5, type=int, help='Warmup epochs for cosine scheduler')
    a_parser.add_argument('--dropout', default=0.1, type=float, help='Dropout rate for regularization (0.0-1.0)')
    a_parser.add_argument('--save_log', default=True, type=bool, help='Save console output to log file in result directory')
    
    return a_parser


def setup_log(result_dir, timestamp, enable=True):
    """
    è®¾ç½®æ—¥å¿—é‡å®šå‘ï¼šå°†æ§åˆ¶å°è¾“å‡ºåŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        result_dir: ç»“æœç›®å½•è·¯å¾„
        timestamp: æ—¶é—´æˆ³
        enable: æ˜¯å¦å¯ç”¨é‡å®šå‘
    
    Returns:
        restore_func: æ¢å¤å‡½æ•°ï¼Œè°ƒç”¨åæ¢å¤æ ‡å‡†è¾“å‡ºå¹¶å…³é—­æ—¥å¿—æ–‡ä»¶
        log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªå¯ç”¨åˆ™è¿”å›None
    """
    log_file_path = os.path.join(result_dir, f'train_{timestamp}.log')
    
    if not enable:
        return lambda: None, None
    
    log_file_obj = open(log_file_path, 'w', encoding='utf-8')
    
    class Tee:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    sys.stdout = Tee(sys.stdout, log_file_obj)
    sys.stderr = Tee(sys.stderr, log_file_obj)
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    
    def restore():
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        log_file_obj.close()
    
    return restore, log_file_path


def main(args):
    # è®¾ç½®éšæœºç§å­
    set_seed(57)
    
    config = get_config()
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›äº†timestampåˆ™ä½¿ç”¨ï¼Œå¦åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
    if args.timestamp:
        timestamp = args.timestamp
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = os.path.join(args.result_dir, timestamp)
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(os.path.join(result_dir, "checkpoints"), exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—é‡å®šå‘
    restore, log_file_path = setup_log(result_dir, timestamp, enable=args.save_log)
    
    Dataset = eval(args.dataset_type)
    # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼šè®­ç»ƒé›†90%ï¼ŒéªŒè¯é›†5%ï¼Œæµ‹è¯•é›†5%
    train_dataset = Dataset(config['SimDataset.data'], args.dataset_type, start_percent=0.0, end_percent=0.9, stage='train', threshold_mode=args.threshold_mode)
    val_dataset = Dataset(config['SimDataset.data'], args.dataset_type, start_percent=0.9, end_percent=0.95, stage='val', threshold_mode=args.threshold_mode)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                             collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                             pin_memory=args.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, 
                           collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                           pin_memory=args.pin_memory)


    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    model = eval(args.model_type)(d_model=args.d_model, dropout=args.dropout).to(args.device)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=args.weight_decay)
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.lr_scheduler == 'cosine':
        from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
        warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=args.warmup_epochs)
        cosine_scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs - args.warmup_epochs, eta_min=args.lr * 0.01)
        scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[args.warmup_epochs])
        print(f"ğŸ“ˆ ä½¿ç”¨Cosineå­¦ä¹ ç‡è°ƒåº¦å™¨ (Warmup={args.warmup_epochs} epochs)")
    elif args.lr_scheduler == 'plateau':
        from torch.optim.lr_scheduler import ReduceLROnPlateau
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)
        print(f"ğŸ“ˆ ä½¿ç”¨Plateauå­¦ä¹ ç‡è°ƒåº¦å™¨ (patience=5)")
    else:
        scheduler = None
        print(f"ğŸ“ˆ ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆå‚æ•°ä»loss.pyçš„LOSS_CONFIGè‡ªåŠ¨è¯»å–ï¼‰
    loss_fn = getattr(loss, args.loss_type)
    print(f"ğŸ“Š ä½¿ç”¨{args.loss_type}æŸå¤±ï¼ˆå‚æ•°ä»loss.pyçš„LOSS_CONFIGè¯»å–ï¼‰")
    
    # åˆ›å»ºæŒ‡æ ‡å‡½æ•°ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼å’Œæ¨¡å¼ï¼‰
    def metrics_fn(pred, target):
        return b_v_metrics(pred, target, mode=args.threshold_mode, threshold=args.metrics_threshold)
    
    # è®­ç»ƒçŠ¶æ€
    best_score = float('inf')  # æŸå¤±è¶Šå°è¶Šå¥½
    best_epoch = 0
    patience_counter = 0
    training_history = []  # å­˜å‚¨è®­ç»ƒå†å²
    
    # ç»„è£…ä¿å­˜è·¯å¾„ï¼ˆä¿å­˜åˆ°result/{timestamp}/ç›®å½•ï¼‰
    best_model_path = os.path.join(result_dir, "checkpoints", f'best_model_{timestamp}.pth')
    train_data_path = os.path.join(result_dir, f'train_{timestamp}.json')
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {args.model_type} + {args.loss_type} | æ•°æ®é›†: {len(train_dataset)}/{len(val_dataset)} | Epochs: {args.epochs}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ”„ Epoch {epoch+1}/{args.epochs} å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒå’ŒéªŒè¯
        visual_dir = os.path.join(result_dir, 'train_visual') if result_dir else None
        train_loss_result = train_epoch(model, train_loader, optimizer, loss_fn, epoch+1, visual_dir, args)
        val_loss_result, val_metrics = validate_epoch(model, val_loader, loss_fn, metrics_fn, args.device)
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°åŸºç¡€æŒ‡æ ‡
        print(f"â±ï¸  Epoch {epoch+1} å®Œæˆï¼Œè€—æ—¶: {epoch_time:.2f}ç§’")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss_result['total']:.4f} | éªŒè¯æŸå¤±: {val_loss_result['total']:.4f}")
        if val_metrics:
            print(f"ğŸ“ˆ éªŒè¯æŒ‡æ ‡: {val_metrics}")
        else:
            print("ğŸ“ˆ éªŒè¯æŒ‡æ ‡: None")
        
        # è®°å½•è®­ç»ƒå†å²ï¼ˆtrain_loss_resultå’Œval_loss_resultå·²åŒ…å«totalå’Œlossesï¼‰
        training_history.append({
            'epoch': epoch + 1,
            'val_metrics': val_metrics,
            'train_loss_result': train_loss_result,
            'val_loss_result': val_loss_result
        })
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜ï¼ˆä½¿ç”¨éªŒè¯æŸå¤±æŒ‡å¯¼ï¼‰
        current_loss = val_loss_result['total']
        is_best = current_loss < best_score  # æŸå¤±è¶Šå°è¶Šå¥½
        
        if is_best:
            best_score = current_loss
            best_epoch = epoch + 1
            patience_counter = 0
            save_model(model, optimizer, epoch + 1, best_score, val_metrics, best_model_path)
            print(f"ğŸ¯ æ–°æœ€ä½³æ¨¡å‹! Val_Loss={best_score:.4f} â­ (è€å¿ƒå€¼é‡ç½®)")
        else:
            patience_counter += 1
            print(f"â³ è€å¿ƒå€¼: {patience_counter}/{args.patience} (Val_Loss={current_loss:.4f})")
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler is not None:
            if args.lr_scheduler == 'plateau':
                scheduler.step(current_loss)
            else:
                scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            print(f"ğŸ“‰ å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"â¹ï¸ æ—©åœè§¦å‘! è¿ç»­ {args.patience} ä¸ªepochæ— æ”¹å–„")
            break
    
    print(f"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} ä¸ªepochï¼ŒéªŒè¯æŸå¤±: {best_score:.4f}")
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    print("\nğŸ“Š ä¿å­˜è®­ç»ƒæ•°æ®...")
    save_training_data(
        training_history=training_history,
        save_path=train_data_path,
        timestamp=timestamp,
        best_model_path=best_model_path,
        args=args,
        config=config
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
    print(f"   - æ¨¡å‹è·¯å¾„: {best_model_path}")
    print(f"   - è®­ç»ƒæ•°æ®: {train_data_path}")
    if args.save_log:
        print(f"   - æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print(f"\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæµ‹è¯•:")
    print(f"   python3 test.py --checkpoint {best_model_path}")
    
    # æ¢å¤æ—¥å¿—é‡å®šå‘
    restore()


def train_epoch(model, train_loader, optimizer, loss_fn, current_epoch, visual_dir=None, args=None):
    model.train()
    total_loss = 0.0
    batch_count = 0
    individual_losses_sum = {}  # ç”¨äºå­˜å‚¨å„ä¸ªæŸå¤±ï¼ˆmixed lossçš„æƒ…å†µï¼‰
    total_batches = len(train_loader)
    
    # æ¯10ä¸ªbatchæˆ–æ¯25%è¿›åº¦æ‰“å°ä¸€æ¬¡
    print_interval = max(1, total_batches // 10)  # è‡³å°‘æ¯10%æ‰“å°ä¸€æ¬¡
    if total_batches < 10:
        print_interval = max(1, total_batches // 4)  # å°æ•°æ®é›†æ—¶æ›´é¢‘ç¹
    
    batch_start_time = time.time()
    
    for batch_idx, batch in enumerate(train_loader):
        src, tgt = batch
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device = args.device
        src = {key: value.to(device) for key, value in src.items()}
        tgt = {key: value.to(device) for key, value in tgt.items()}
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
        
        # æ¯100ä¸ªbatchç”»ä¸€æ¬¡å›¾
        if visual_dir and batch_idx % 100 == 0:
            os.makedirs(visual_dir, exist_ok=True)
            try:
                save_path = os.path.join(visual_dir, f'train_{current_epoch}_{batch_idx}.png')
                plot_single_sample(src, thresholds_pred, tgt["thresholds"], save_path, epoch=current_epoch, threshold=args.metrics_threshold)
            except Exception as e:
                print(f"âš ï¸  è®­ç»ƒæ ·æœ¬å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        
        # è®¡ç®—æŸå¤±
        loss_result = loss_fn(thresholds_pred, tgt["thresholds"])
        
        # å¤„ç†mixed lossè¿”å›å­—å…¸çš„æƒ…å†µ
        if isinstance(loss_result, dict) and 'total' in loss_result:
            loss = loss_result['total']
            for k, v in loss_result.get('losses', {}).items():
                individual_losses_sum[k] = individual_losses_sum.get(k, 0.0) + v.item()
        else:
            loss = loss_result
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        if args.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
        
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:
            progress = (batch_idx + 1) / total_batches * 100
            batch_time = time.time() - batch_start_time
            avg_batch_time = batch_time / print_interval
            current_avg_loss = total_loss / batch_count
            
            print(f"  ğŸ“¦ Batch {batch_idx+1}/{total_batches} ({progress:.1f}%) | "
                  f"Loss: {current_avg_loss:.4f} | "
                  f"é€Ÿåº¦: {avg_batch_time:.2f}s/batch")
            
            batch_start_time = time.time()
        
    avg_loss = total_loss / batch_count
    # ç»Ÿä¸€è¿”å›å­—å…¸æ ¼å¼ï¼šåŒ…å«æ€»æŸå¤±å’Œå„ä¸ªæŸå¤±çš„å¹³å‡å€¼ï¼Œå¹¶è½¬æ¢ä¸º float
    avg_individual_losses = {k: float(v / batch_count) for k, v in individual_losses_sum.items()} if individual_losses_sum else {}
    return {'total': float(avg_loss), 'losses': avg_individual_losses}


def validate_epoch(model, val_loader, loss_fn, metrics_fn, device):
    model.eval()
    val_loss = 0.0
    val_batch_count = 0
    individual_losses_sum = {}  # ç”¨äºå­˜å‚¨å„ä¸ªæŸå¤±ï¼ˆmixed lossçš„æƒ…å†µï¼‰
    total_val_batches = len(val_loader)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡
    all_predictions = []
    all_targets = []
    
    print(f"  ğŸ” å¼€å§‹éªŒè¯ ({total_val_batches} batches)...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            src, tgt = batch
            # ç§»åŠ¨åˆ°è®¾å¤‡
            src = {key: value.to(device) for key, value in src.items()}
            tgt = {key: value.to(device) for key, value in tgt.items()}
            
            thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
            loss_result = loss_fn(thresholds_pred, tgt["thresholds"])
            
            # å¤„ç†mixed lossè¿”å›å­—å…¸çš„æƒ…å†µ
            if isinstance(loss_result, dict) and 'total' in loss_result:
                loss = loss_result['total']
                for k, v in loss_result.get('losses', {}).items():
                    individual_losses_sum[k] = individual_losses_sum.get(k, 0.0) + v.item()
            else:
                loss = loss_result
            
            val_loss += loss.item()
            val_batch_count += 1
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            all_predictions.append(thresholds_pred)
            all_targets.append(tgt["thresholds"])
            
            # éªŒè¯è¿›åº¦æç¤ºï¼ˆåªåœ¨éªŒè¯é›†è¾ƒå¤§æ—¶æ˜¾ç¤ºï¼‰
            if total_val_batches > 5 and (batch_idx + 1) % max(1, total_val_batches // 5) == 0:
                progress = (batch_idx + 1) / total_val_batches * 100
                print(f"    ğŸ” éªŒè¯è¿›åº¦: {batch_idx+1}/{total_val_batches} ({progress:.0f}%)")
    
    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    if all_predictions:
        all_pred = torch.cat(all_predictions, dim=0)
        all_true = torch.cat(all_targets, dim=0)
        
        # ä½¿ç”¨ç»¼åˆè¯„ä»·æŒ‡æ ‡
        val_metrics = metrics_fn(all_pred, all_true)
        
        avg_val_loss = val_loss / val_batch_count
        # ç»Ÿä¸€è¿”å›å­—å…¸æ ¼å¼ï¼šåŒ…å«æ€»æŸå¤±å’Œå„ä¸ªæŸå¤±çš„å¹³å‡å€¼ï¼Œå¹¶è½¬æ¢ä¸º float
        avg_individual_losses = {k: float(v / val_batch_count) for k, v in individual_losses_sum.items()} if individual_losses_sum else {}
        val_loss_result = {'total': float(avg_val_loss), 'losses': avg_individual_losses}
        print(f"  âœ… éªŒè¯å®Œæˆ: {val_batch_count} batches")
        return val_loss_result, val_metrics
    else:
        raise RuntimeError("éªŒè¯é˜¶æ®µæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æŒ‡æ ‡å‡½æ•°")


def save_training_data(training_history, save_path, timestamp, best_model_path, args=None, config=None):
    """ä¿å­˜è®­ç»ƒæ•°æ®ä¸ºJSONæ ¼å¼"""
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    training_data = {
        'timestamp': timestamp,
        'total_epochs': len(training_history),
        'best_model_path': best_model_path,
        'training_history': training_history
    }
    
    # æ·»åŠ argsé…ç½®ï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
    if args is not None:
        training_data['config_args'] = vars(args)
    
    # æ·»åŠ configé…ç½®ï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
    if config is not None:
        training_data['config'] = config.to_dict() if hasattr(config, 'to_dict') else config
    
    # ä¿å­˜ä¸ºJSON
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: {save_path}")


def save_model(model, optimizer, epoch, best_score, val_metrics, save_path):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_score': best_score,
        'val_metrics': val_metrics
    }, save_path)





if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)

