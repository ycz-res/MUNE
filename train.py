"""
è®­ç»ƒæ¨¡å—
åŒ…å«è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ¨¡å‹ä¿å­˜ç­‰åŠŸèƒ½
"""

from torch.utils.data import DataLoader
import torch
import torch.optim as optim
import argparse
import os
from datetime import datetime
import numpy as np
import time
import json
import pandas as pd

from dataset import Sim
from config import get_config
from model import Linear, CNN, LSTM
from loss import ce, focal_ce, thr
from visualization import MUThresholdVisualizer
from utils import set_seed
from metrics import b_v_metrics


def get_args_parser():
    a_parser = argparse.ArgumentParser('MU Threshold Prediction Training', add_help=False)
    a_parser.add_argument('--batch_size', default=2, type=int, help='Batch size for training')
    a_parser.add_argument('--epochs', default=10, type=int, help='Number of training epochs')
    a_parser.add_argument('--shuffle', default=True, type=bool, help='Shuffle training data')
    a_parser.add_argument('--num_workers', default=0, type=int, help='Number of data loading workers')
    a_parser.add_argument('--pin_memory', default=False, type=bool, help='Pin memory for data loading')
    a_parser.add_argument('--device', default='cpu', type=str, help='Device to use (cpu/cuda)')
    a_parser.add_argument('--lr', default=1e-5, type=float, help='Learning rate')
    a_parser.add_argument('--weight_decay', default=1e-3, type=float, help='Weight decay')
    a_parser.add_argument('--patience', default=5, type=int, help='Early stopping patience')
    a_parser.add_argument('--loss_type', default='ce', choices=['thr', 'focal', 'ce'], help='Loss function type: thr=threshold loss, focal=focal loss, ce=cross entropy')
    a_parser.add_argument('--model_type', default='Linear', choices=['Linear', 'CNN', 'LSTM'], help='Model architecture type')
    a_parser.add_argument('--save_best', default=True, type=bool, help='Save best model')
    a_parser.add_argument('--save_dir', default='checkpoints', type=str, help='Directory to save models')
    a_parser.add_argument('--threshold_mode', default='binary', choices=['value', 'binary'], help='Threshold output mode: binary=0/1 mask, value=actual threshold values')
    a_parser.add_argument('--dataset_type', default='Sim', choices=['Sim'], help='Dataset type')
    a_parser.add_argument('--metrics_threshold', default=0.1, type=float, help='Threshold for metrics calculation (0.1-0.3 recommended for sparse data)')
    a_parser.add_argument('--use_weighted_loss', default=True, type=bool, help='Use weighted loss for imbalanced data')
    a_parser.add_argument('--pos_weight', default=50.0, type=float, help='Positive class weight for weighted loss')
    
    return a_parser

def main(args):
    # è®¾ç½®éšæœºç§å­
    set_seed(57)
    
    config = get_config()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    curves_dir = f"plot/training_curves_{timestamp}"
    os.makedirs(curves_dir, exist_ok=True)
    os.makedirs(args.save_dir, exist_ok=True)
    
    Dataset = eval(args.dataset_type)
    # æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼šè®­ç»ƒé›†90%ï¼ŒéªŒè¯é›†5%ï¼Œæµ‹è¯•é›†5%
    train_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.0, end_percent=0.9, stage='train', threshold_mode=args.threshold_mode)
    val_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.9, end_percent=0.95, stage='val', threshold_mode=args.threshold_mode)
    test_dataset = Dataset(config['SimDataset.data'], 'sim', start_percent=0.95, end_percent=1.0, stage='test', threshold_mode=args.threshold_mode)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                             collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                             pin_memory=args.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, 
                           collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                           pin_memory=args.pin_memory)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                            collate_fn=Dataset.collate_fn, num_workers=args.num_workers, 
                            pin_memory=args.pin_memory)

    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    model = eval(args.model_type)(d_model=64).to(args.device)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=args.weight_decay)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆæ”¯æŒåŠ æƒï¼‰
    if args.use_weighted_loss and args.loss_type == 'ce':
        pos_weight_tensor = torch.tensor(args.pos_weight, device=args.device)
        def loss_fn(pred, target):
            return ce(pred, target, pos_weight=pos_weight_tensor)
    else:
        loss_fn = eval(args.loss_type)
    
    # åˆ›å»ºæŒ‡æ ‡å‡½æ•°ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼ï¼‰
    def metrics_fn(pred, target):
        return b_v_metrics(pred, target, threshold=args.metrics_threshold)
    
    # è®­ç»ƒçŠ¶æ€
    best_score = -float('inf')
    best_epoch = 0
    patience_counter = 0
    training_history = []  # å­˜å‚¨è®­ç»ƒå†å²
    
    # åˆ›å»ºæ—¥å¿—ä¿å­˜ç›®å½•
    log_dir = os.path.join(curves_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—æ•°æ®å­˜å‚¨
    prediction_logs = {
        'train': [],
        'val': [],
        'metadata': {
            'timestamp': timestamp,
            'model_type': args.model_type,
            'loss_type': args.loss_type,
            'threshold_mode': args.threshold_mode,
            'metrics_threshold': args.metrics_threshold,
            'batch_size': args.batch_size,
            'lr': args.lr
        }
    }
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {args.model_type} + {args.loss_type} | æ•°æ®é›†: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)} | Epochs: {args.epochs}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"\nğŸ”„ Epoch {epoch+1}/{args.epochs} å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒå’ŒéªŒè¯
        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, args.device, epoch+1, args.epochs, prediction_logs)
        val_loss, val_metrics, val_pred, val_target = validate_epoch(model, val_loader, loss_fn, metrics_fn, args.device, prediction_logs)
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°åŸºç¡€æŒ‡æ ‡
        print(f"â±ï¸  Epoch {epoch+1} å®Œæˆï¼Œè€—æ—¶: {epoch_time:.2f}ç§’")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        if val_metrics:
            print(f"ğŸ“ˆ éªŒè¯æŒ‡æ ‡: {val_metrics}")
        else:
            print("ğŸ“ˆ éªŒè¯æŒ‡æ ‡: None")
        
        # è®°å½•è®­ç»ƒå†å²
        training_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_metrics': val_metrics,
            'val_pred': val_pred,
            'val_target': val_target
        })
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜
        current_score = val_metrics.get('score', 0) if val_metrics else 0
        is_best = current_score > best_score
        
        if is_best:
            best_score = current_score
            best_epoch = epoch + 1
            patience_counter = 0
            save_model(model, optimizer, epoch + 1, best_score, val_metrics, args.save_dir, timestamp)
            print(f"ğŸ¯ æ–°æœ€ä½³æ¨¡å‹! Score={best_score:.4f} â­ (è€å¿ƒå€¼é‡ç½®)")
        else:
            patience_counter += 1
            print(f"â³ è€å¿ƒå€¼: {patience_counter}/{args.patience} (Score={current_score:.4f})")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= args.patience:
            print(f"â¹ï¸ æ—©åœè§¦å‘! è¿ç»­ {args.patience} ä¸ªepochæ— æ”¹å–„")
            break
    
    print(f"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} ä¸ªepochï¼Œç»¼åˆåˆ†æ•°: {best_score:.4f}")
    
    # ä¿å­˜é¢„æµ‹æ—¥å¿—
    save_prediction_logs(prediction_logs, log_dir, timestamp)
    
    # æµ‹è¯•é˜¶æ®µ
    load_best_model(model, args.save_dir, timestamp)
    print("ğŸ§ª æµ‹è¯•é˜¶æ®µ")
    test_loss, test_metrics, _, _ = validate_epoch(model, test_loader, loss_fn, metrics_fn, args.device, None)
    
    # æ‰“å°æµ‹è¯•æŒ‡æ ‡
    print(f"âœ… æµ‹è¯•å®Œæˆ, å¹³å‡æŸå¤±: {test_loss:.6f}")
    if test_metrics:
        print(f"test_metrics: {test_metrics}")
    else:
        print("test_metrics: None")
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    generate_training_report(
        training_history=training_history,
        test_loss=test_loss,
        test_metrics=test_metrics,
        best_epoch=best_epoch,
        best_score=best_score,
        model_info={
            "æ¨¡å‹ç±»å‹": args.model_type,
            "éšè—ç»´åº¦": 64,
            "ä¼˜åŒ–å™¨": "AdamW",
            "å­¦ä¹ ç‡": args.lr,
            "æƒé‡è¡°å‡": args.weight_decay,
            "æŸå¤±å‡½æ•°": args.loss_type,
            "æ—©åœè€å¿ƒ": args.patience
        },
        dataset_info={
            "è®­ç»ƒæ ·æœ¬æ•°": len(train_dataset),
            "éªŒè¯æ ·æœ¬æ•°": len(val_dataset),
            "æµ‹è¯•æ ·æœ¬æ•°": len(test_dataset),
            "æ‰¹æ¬¡å¤§å°": args.batch_size
        },
        save_dir=curves_dir,
        timestamp=timestamp
    )


def train_epoch(model, train_loader, optimizer, loss_fn, device, current_epoch, total_epochs, prediction_logs=None):
    model.train()
    total_loss = 0.0
    batch_count = 0
    total_batches = len(train_loader)
    
    # æ¯10ä¸ªbatchæˆ–æ¯25%è¿›åº¦æ‰“å°ä¸€æ¬¡
    print_interval = max(1, total_batches // 10)  # è‡³å°‘æ¯10%æ‰“å°ä¸€æ¬¡
    if total_batches < 10:
        print_interval = max(1, total_batches // 4)  # å°æ•°æ®é›†æ—¶æ›´é¢‘ç¹
    
    batch_start_time = time.time()
    
    for batch_idx, batch in enumerate(train_loader):
        src, tgt = batch
        # ç§»åŠ¨åˆ°è®¾å¤‡
        src = {key: value.to(device) for key, value in src.items()}
        tgt = {key: value.to(device) for key, value in tgt.items()}
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
        
        # è®¡ç®—æŸå¤±
        loss = loss_fn(thresholds_pred, tgt["thresholds"])
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        batch_count += 1
        
        # å®šæœŸæ‰“å°è¿›åº¦å’Œé¢„æµ‹å¯¹æ¯”
        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == total_batches:
            progress = (batch_idx + 1) / total_batches * 100
            batch_time = time.time() - batch_start_time
            avg_batch_time = batch_time / print_interval
            current_avg_loss = total_loss / batch_count
            
            print(f"  ğŸ“¦ Batch {batch_idx+1}/{total_batches} ({progress:.1f}%) | "
                  f"Loss: {current_avg_loss:.4f} | "
                  f"é€Ÿåº¦: {avg_batch_time:.2f}s/batch")
            
            # è¾“å‡ºå½“å‰batchçš„é¢„æµ‹å¯¹æ¯”
            batch_log = print_batch_predictions(thresholds_pred, tgt["thresholds"], batch_idx+1, current_epoch, "è®­ç»ƒ", 0.1)
            # æ”¶é›†æ—¥å¿—æ•°æ®
            if prediction_logs is not None:
                prediction_logs['train'].append(batch_log)
            
            batch_start_time = time.time()
        
    avg_loss = total_loss / batch_count
    return avg_loss


def validate_epoch(model, val_loader, loss_fn, metrics_fn, device, prediction_logs=None):
    model.eval()
    val_loss = 0.0
    val_batch_count = 0
    total_val_batches = len(val_loader)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼ç”¨äºè®¡ç®—æŒ‡æ ‡
    all_predictions = []
    all_targets = []
    
    print(f"  ğŸ” å¼€å§‹éªŒè¯ ({total_val_batches} batches)...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            src, tgt = batch
            # ç§»åŠ¨åˆ°è®¾å¤‡
            src = {key: value.to(device) for key, value in src.items()}
            tgt = {key: value.to(device) for key, value in tgt.items()}
            
            thresholds_pred = model(src["cmap"])  # æ¨¡å‹åªè¾“å‡ºé˜ˆå€¼é¢„æµ‹
            loss = loss_fn(thresholds_pred, tgt["thresholds"])
            
            val_loss += loss.item()
            val_batch_count += 1
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            all_predictions.append(thresholds_pred)
            all_targets.append(tgt["thresholds"])
            
            # éªŒè¯è¿›åº¦æç¤ºå’Œé¢„æµ‹å¯¹æ¯”ï¼ˆåªåœ¨éªŒè¯é›†è¾ƒå¤§æ—¶æ˜¾ç¤ºï¼‰
            if total_val_batches > 5 and (batch_idx + 1) % max(1, total_val_batches // 5) == 0:
                progress = (batch_idx + 1) / total_val_batches * 100
                print(f"    ğŸ” éªŒè¯è¿›åº¦: {batch_idx+1}/{total_val_batches} ({progress:.0f}%)")
                
                # è¾“å‡ºå½“å‰batchçš„é¢„æµ‹å¯¹æ¯”
                batch_log = print_batch_predictions(thresholds_pred, tgt["thresholds"], batch_idx+1, 0, "éªŒè¯", 0.1)
                # æ”¶é›†æ—¥å¿—æ•°æ®
                if prediction_logs is not None:
                    prediction_logs['val'].append(batch_log)
    
    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    if all_predictions:
        all_pred = torch.cat(all_predictions, dim=0)
        all_true = torch.cat(all_targets, dim=0)
        
        # ä½¿ç”¨ç»¼åˆè¯„ä»·æŒ‡æ ‡
        val_metrics = metrics_fn(all_pred, all_true)
        
        avg_val_loss = val_loss / val_batch_count
        print(f"  âœ… éªŒè¯å®Œæˆ: {val_batch_count} batches")
        return avg_val_loss, val_metrics, all_pred, all_true
    else:
        raise RuntimeError("éªŒè¯é˜¶æ®µæ— æ³•è®¡ç®—æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æŒ‡æ ‡å‡½æ•°")


def print_batch_predictions(pred: torch.Tensor, target: torch.Tensor, batch_idx: int, epoch: int, stage: str, threshold: float = 0.1):
    """
    è¾“å‡ºbatchçº§åˆ«çš„é¢„æµ‹å¯¹æ¯”ä¿¡æ¯å¹¶è¿”å›æ—¥å¿—æ•°æ®
    
    Args:
        pred: é¢„æµ‹ç»“æœ (batch_size, 500)
        target: çœŸå®æ ‡ç­¾ (batch_size, 500)
        batch_idx: batchç´¢å¼•
        epoch: epochç´¢å¼•
        stage: é˜¶æ®µï¼ˆè®­ç»ƒ/éªŒè¯ï¼‰
        threshold: äºŒå€¼åŒ–é˜ˆå€¼
    
    Returns:
        dict: åŒ…å«é¢„æµ‹å¯¹æ¯”æ•°æ®çš„å­—å…¸
    """
    # å°†é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡å¹¶äºŒå€¼åŒ–
    prob = torch.sigmoid(pred)
    pred_binary = (prob >= threshold).float()
    
    batch_size = pred.shape[0]
    
    print(f"    ğŸ“Š {stage} Batch {batch_idx} é¢„æµ‹å¯¹æ¯”:")
    
    # å­˜å‚¨æ ·æœ¬çº§æ•°æ®
    sample_data = []
    
    # è¾“å‡ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†å¯¹æ¯”
    for i in range(min(3, batch_size)):
        true_count = int(target[i].sum().item())
        pred_count = int(pred_binary[i].sum().item())
        
        # æ‰¾åˆ°çœŸå®å’Œé¢„æµ‹çš„é˜ˆå€¼ä½ç½®
        true_positions = torch.where(target[i] > 0)[0].cpu().numpy()
        pred_positions = torch.where(pred_binary[i] > 0)[0].cpu().numpy()
        
        print(f"      æ ·æœ¬ {i+1}: çœŸå®MUæ•°é‡={true_count}, é¢„æµ‹MUæ•°é‡={pred_count}")
        
        if len(true_positions) > 0:
            print(f"        çœŸå®é˜ˆå€¼ä½ç½®: {true_positions[:10]}{'...' if len(true_positions) > 10 else ''}")
        else:
            print(f"        çœŸå®é˜ˆå€¼ä½ç½®: æ— ")
            
        if len(pred_positions) > 0:
            print(f"        é¢„æµ‹é˜ˆå€¼ä½ç½®: {pred_positions[:10]}{'...' if len(pred_positions) > 10 else ''}")
        else:
            print(f"        é¢„æµ‹é˜ˆå€¼ä½ç½®: æ— ")
        
        # è®¡ç®—é‡å 
        if len(true_positions) > 0:
            overlap = len(set(true_positions) & set(pred_positions))
            overlap_ratio = overlap / len(true_positions)
            print(f"        é‡å ä½ç½®æ•°: {overlap}/{len(true_positions)} (é‡å ç‡: {overlap_ratio:.3f})")
        else:
            overlap = 0
            overlap_ratio = 1.0
            print(f"        é‡å ä½ç½®æ•°: 0/0 (é‡å ç‡: 1.000)")
        
        # å­˜å‚¨æ ·æœ¬æ•°æ®
        sample_data.append({
            'sample_idx': i,
            'true_mu_count': true_count,
            'pred_mu_count': pred_count,
            'true_positions': true_positions.tolist(),
            'pred_positions': pred_positions.tolist(),
            'overlap_count': overlap,
            'overlap_ratio': overlap_ratio,
            'true_prob_values': prob[i][true_positions].cpu().numpy().tolist() if len(true_positions) > 0 else [],
            'pred_prob_values': prob[i][pred_positions].cpu().numpy().tolist() if len(pred_positions) > 0 else []
        })
    
    if batch_size > 3:
        print(f"      ... è¿˜æœ‰ {batch_size - 3} ä¸ªæ ·æœ¬")
    
    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    true_counts = target.sum(dim=1).cpu().numpy()
    pred_counts = pred_binary.sum(dim=1).cpu().numpy()
    
    avg_true = np.mean(true_counts)
    avg_pred = np.mean(pred_counts)
    mae = np.mean(np.abs(pred_counts - true_counts))
    
    print(f"    ğŸ“ˆ Batchç»Ÿè®¡: å¹³å‡çœŸå®MU={avg_true:.2f}, å¹³å‡é¢„æµ‹MU={avg_pred:.2f}, MAE={mae:.2f}")
    
    # è¿”å›æ—¥å¿—æ•°æ®
    batch_log = {
        'epoch': epoch,
        'batch_idx': batch_idx,
        'stage': stage,
        'threshold': threshold,
        'batch_size': batch_size,
        'batch_stats': {
            'avg_true_mu': float(avg_true),
            'avg_pred_mu': float(avg_pred),
            'mae': float(mae),
            'true_counts': true_counts.tolist(),
            'pred_counts': pred_counts.tolist()
        },
        'sample_details': sample_data,
        'timestamp': datetime.now().isoformat()
    }
    
    return batch_log


def save_prediction_logs(prediction_logs, log_dir, timestamp):
    """
    ä¿å­˜é¢„æµ‹æ—¥å¿—æ•°æ®ä¸ºå¤šç§æ ¼å¼
    
    Args:
        prediction_logs: é¢„æµ‹æ—¥å¿—æ•°æ®
        log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    # 1. ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆå®Œæ•´æ•°æ®ï¼‰
    json_path = os.path.join(log_dir, f'prediction_logs_{timestamp}.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(prediction_logs, f, ensure_ascii=False, indent=2)
    
    # 2. ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºå¯è§†åŒ–ï¼‰
    csv_data = []
    for stage in ['train', 'val']:
        for log_entry in prediction_logs[stage]:
            for sample in log_entry['sample_details']:
                csv_data.append({
                    'epoch': log_entry['epoch'],
                    'batch_idx': log_entry['batch_idx'],
                    'stage': log_entry['stage'],
                    'sample_idx': sample['sample_idx'],
                    'true_mu_count': sample['true_mu_count'],
                    'pred_mu_count': sample['pred_mu_count'],
                    'overlap_count': sample['overlap_count'],
                    'overlap_ratio': sample['overlap_ratio'],
                    'true_positions_count': len(sample['true_positions']),
                    'pred_positions_count': len(sample['pred_positions']),
                    'timestamp': log_entry['timestamp']
                })
    
    if csv_data:
        df = pd.DataFrame(csv_data)
        csv_path = os.path.join(log_dir, f'prediction_summary_{timestamp}.csv')
        df.to_csv(csv_path, index=False)
        
        # 3. ä¿å­˜batchçº§ç»Ÿè®¡CSV
        batch_stats_data = []
        for stage in ['train', 'val']:
            for log_entry in prediction_logs[stage]:
                batch_stats_data.append({
                    'epoch': log_entry['epoch'],
                    'batch_idx': log_entry['batch_idx'],
                    'stage': log_entry['stage'],
                    'avg_true_mu': log_entry['batch_stats']['avg_true_mu'],
                    'avg_pred_mu': log_entry['batch_stats']['avg_pred_mu'],
                    'mae': log_entry['batch_stats']['mae'],
                    'batch_size': log_entry['batch_size'],
                    'timestamp': log_entry['timestamp']
                })
        
        df_batch = pd.DataFrame(batch_stats_data)
        batch_csv_path = os.path.join(log_dir, f'batch_stats_{timestamp}.csv')
        df_batch.to_csv(batch_csv_path, index=False)
    
    print(f"ğŸ“ é¢„æµ‹æ—¥å¿—å·²ä¿å­˜:")
    print(f"   JSONæ ¼å¼: {json_path}")
    if csv_data:
        print(f"   æ ·æœ¬çº§CSV: {csv_path}")
        print(f"   Batchçº§CSV: {batch_csv_path}")


def generate_training_report(training_history, test_loss, test_metrics, best_epoch, best_score, 
                           model_info, dataset_info, save_dir, timestamp):
    """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨"""
    visualizer = MUThresholdVisualizer(save_dir)
    
    # æ›´æ–°è®­ç»ƒå†å²
    for epoch_data in training_history:
        visualizer.update_epoch(
            epoch_data['epoch'], 
            epoch_data['train_loss'], 
            epoch_data['val_loss'],
            test_loss=test_loss,
            val_metrics=epoch_data.get('val_metrics'),
            val_pred=epoch_data.get('val_pred'),
            val_target=epoch_data.get('val_target')
        )
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    visualizer.generate_comprehensive_report(
        test_loss=test_loss,
        model_info=model_info,
        dataset_info=dataset_info
    )
    
    print(f"ğŸ“Š è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {save_dir}")


def save_model(model, optimizer, epoch, best_score, val_metrics, save_dir, timestamp):
    """ä¿å­˜æœ€ä½³æ¨¡å‹"""
    model_path = os.path.join(save_dir, f'best_model_{timestamp}.pth')
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_score': best_score,
        'val_metrics': val_metrics
    }, model_path)


def load_best_model(model, save_dir, timestamp):
    """åŠ è½½æœ€ä½³æ¨¡å‹"""
    model_path = os.path.join(save_dir, f'best_model_{timestamp}.pth')
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path)
        model.load_state_dict(checkpoint['model_state_dict'])



if __name__ == '__main__':
    parser = get_args_parser()
    args = parser.parse_args()
    main(args)

