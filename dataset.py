from py_compile import main
import torch
import numpy as np
from torch.utils.data import Dataset
from typing import List, Tuple, Dict, Any
from utils import load_mat_data
import h5py



class Sim(Dataset):
    """ä»¿çœŸæ•°æ®é›†ç±»"""
    
    def __init__(self, data_path: str, data_type: str = 'sim', start_percent: float = 0.0, 
                 end_percent: float = 1.0, stage: str = 'train', threshold_mode: str = 'binary'):
        self.data_path = data_path
        self.data_type = data_type
        self.start_percent = start_percent
        self.end_percent = end_percent
        self.stage = stage
        self.threshold_mode = threshold_mode
        
        if stage not in ['train', 'val', 'test']:
            raise ValueError("é˜¶æ®µæ ‡ç­¾å¿…é¡»æ˜¯ 'train', 'val', æˆ– 'test'")
        if not (0.0 <= start_percent <= 1.0 and 0.0 <= end_percent <= 1.0):
            raise ValueError("ç™¾åˆ†æ¯”å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´")
        if start_percent >= end_percent:
            raise ValueError("èµ·å§‹ç™¾åˆ†æ¯”å¿…é¡»å°äºç»“æŸç™¾åˆ†æ¯”")
        
        # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åŠ è½½æ–¹æ³•
        self.data_dict = self.__load_data(self.data_type)
        
        # æå–æ•°æ®
        self.cmap_amplitudes = self.data_dict['data']       # (N, 500) - CMAPå¹…å€¼æ•°æ®
        self.mu_count_labels = self.data_dict['label_num'] # (N,) - è¿åŠ¨å•ä½æ•°é‡æ ‡ç­¾
        self.mu_thresholds = self.data_dict['muThr']  # (N, 500) - è¿åŠ¨å•ä½é˜ˆå€¼ä½ç½®
        
        # è®¡ç®—æ•°æ®èŒƒå›´
        self.total_samples = self.cmap_amplitudes.shape[0]
        self.start_idx = int(self.total_samples * start_percent)
        self.end_idx = int(self.total_samples * end_percent)
        self.num_samples = self.end_idx - self.start_idx
        
        print(f"æ•°æ®é›†ä¿¡æ¯ - å½“å‰é˜¶æ®µ: {self.stage}: æ€»æ ·æœ¬æ•°={self.total_samples}, "
              f"ä½¿ç”¨èŒƒå›´=[{self.start_idx}:{self.end_idx}], å®é™…æ ·æœ¬æ•°={self.num_samples}")
    
    def __load_data(self, data_type: str):
        if data_type == 'sim':
            return self.__load_sim_data()
        elif data_type == 'real':
            return self.__load_real_data()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")

    def __load_sim_data(self):
        """åŠ è½½ä»¿çœŸæ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†ï¼ˆè‡ªåŠ¨ä¿®å¤MATLABè½¬ç½®ç»´åº¦ï¼‰"""
        import numpy as np

        mat_data = load_mat_data(self.data_path, lazy=False)
        print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {self.data_path}")
        print(f"ğŸ”‘ åŒ…å«å˜é‡: {list(mat_data.keys())}")

        # ========== Step 1. ä¿®æ­£ç»´åº¦é¡ºåº ==========
        data = np.array(mat_data["data"])
        label_num = np.array(mat_data["label_num"]).squeeze()
        muThr = np.array(mat_data["muThr"])

        # âš ï¸ å¦‚æœç»´åº¦æ˜¯ (2,500,780000) åˆ™è¯´æ˜è¢«è½¬ç½®äº†
        if data.shape[0] < data.shape[-1]:
            print(f"âš™ï¸ æ£€æµ‹åˆ°ç»´åº¦åè½¬: data.shape={data.shape} â†’ è‡ªåŠ¨è½¬ç½®ä¸­...")
            data = np.transpose(data, (2, 1, 0))  # (2,500,780000) â†’ (780000,500,2)

        if muThr.shape[0] < muThr.shape[-1]:
            print(f"âš™ï¸ muThr è½¬ç½®: {muThr.shape} â†’ {muThr.T.shape}")
            muThr = muThr.T  # (160,780000) â†’ (780000,160)

        if label_num.ndim == 2 and label_num.shape[0] < label_num.shape[1]:
            print(f"âš™ï¸ label_num è½¬ç½®: {label_num.shape} â†’ {label_num.T.shape}")
            label_num = label_num.T.squeeze()  # (1,780000) â†’ (780000,)

        # ========== Step 2. å½’ä¸€åŒ–CMAPå¹…å€¼æ•°æ® ==========
        cmap_normalized = self._normalize_cmap_data(data)  # (N,500)

        # ========== Step 3. åŠ è½½è¿åŠ¨å•ä½æ•°é‡æ ‡ç­¾ ==========
        mu_counts = label_num.astype(np.float32)  # (N,)

        # ========== Step 4. åŠ è½½å¹¶æ˜ å°„é˜ˆå€¼ ==========
        mu_thresholds_raw = muThr.astype(np.float32)
        mu_thresholds_aligned = self._map_mu_thresholds(data, mu_thresholds_raw)  # (N,500)

        # ========== Step 5. è¾“å‡ºç»“æœ ==========
        result = {
            "data": cmap_normalized,         # (N,500)
            "label_num": mu_counts,          # (N,)
            "muThr": mu_thresholds_aligned   # (N,500)
        }

        print("\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆ:")
        print(f"  - æ ·æœ¬æ•°é‡: {len(cmap_normalized)}")
        print(f"  - CMAPæ•°æ®å½¢çŠ¶: {cmap_normalized.shape}")
        print(f"  - MUæ•°é‡èŒƒå›´: [{mu_counts.min():.1f}, {mu_counts.max():.1f}]")
        print(f"  - é˜ˆå€¼çŸ©é˜µå½¢çŠ¶: {mu_thresholds_aligned.shape}\n")

        return result
    
    def _normalize_cmap_data(self, data):
        """
        å¯¹CMAPæ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        
        Args:
            data: åŸå§‹CMAPæ•°æ®ï¼Œå½¢çŠ¶ä¸º(N, 500, 2)ï¼Œå…¶ä¸­æœ€åä¸€ç»´ä¸º[xåæ ‡, yå¹…å€¼]
            
        Returns:
            Y_norm: å½’ä¸€åŒ–åçš„yå€¼æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N, 500)
            
        å¤„ç†æ­¥éª¤:
            1. æŒ‰xåæ ‡æ’åº 
            2. å¯¹yå¹…å€¼è¿›è¡Œ[0,1]å½’ä¸€åŒ–
        """
        N, P, _ = data.shape
        Y_norm = np.zeros((N, P), dtype=np.float32)

        for i in range(N):
            x, y = data[i, :, 0], data[i, :, 1]
            # æŒ‰xåæ ‡æ’åº
            idx = np.argsort(x)
            y = y[idx]
            # yå½’ä¸€åŒ–åˆ°[0,1]
            y = (y - y.min()) / (y.max() - y.min() + 1e-8)
            Y_norm[i] = y

        return Y_norm

    def _map_mu_thresholds(self, data, muThr):
        """
        å°†æ¯ä¸ªæ ·æœ¬çš„ MU é˜ˆå€¼ (muThr) æ˜ å°„åˆ°å¯¹åº”çš„ x è½´ä½ç½® (500ç»´)ã€‚

        å‚æ•°:
            data: (N, 500, 2)
                æ¯ä¸ªæ ·æœ¬çš„ç”µåˆºæ¿€åºåˆ—å’Œå¹…å€¼ã€‚
                data[n, :, 0] è¡¨ç¤ºåˆºæ¿€ç”µæµåºåˆ— xï¼ˆå•ä½ mAï¼‰
            muThr: (N, 160)
                æ¯ä¸ªæ ·æœ¬çš„è¿åŠ¨å•ä½é˜ˆå€¼åˆ†å¸ƒï¼ˆmAï¼‰ï¼Œ0 è¡¨ç¤ºæ— æ•ˆå¡«å……ã€‚

        è¾“å‡º:
            thr_matrix: (N, 500)
                æ¯ä¸ªæ ·æœ¬çš„ 500 ç»´é˜ˆå€¼æ˜ å°„ç»“æœï¼š
                    - è‹¥ threshold_mode == 'binary' â†’ 0/1 æ©ç 
                    - è‹¥ threshold_mode == 'value' â†’ å®é™…é˜ˆå€¼
        """
        N, P, _ = data.shape
        thr_matrix = np.zeros((N, P), dtype=np.float32)

        for n in range(N):
            # ç”µåˆºæ¿€åæ ‡ï¼ˆå•è°ƒé€’å¢ï¼‰
            x = data[n, :, 0]  # (500,)
            thr_vector = np.zeros(P, dtype=np.float32)

            # æå–è¯¥æ ·æœ¬æœ‰æ•ˆé˜ˆå€¼ï¼šå»0 â†’ æ’åº â†’ å»é‡
            mu_vals = muThr[n][muThr[n] > 0]
            if mu_vals.size == 0:
                thr_matrix[n] = thr_vector
                continue

            mu_vals = np.unique(np.sort(mu_vals))  # ä¿è¯é€’å¢é¡ºåºä¸ç”Ÿç†ä¸€è‡´

            # å°†æ¯ä¸ªé˜ˆå€¼æ˜ å°„åˆ° x è½´æœ€è¿‘ä½ç½®
            for val in mu_vals:
                idx = np.searchsorted(x, val)  # æ‰¾åˆ°ç¬¬ä¸€ä¸ª â‰¥ val çš„ä½ç½®
                if idx < P:  # åªåœ¨æœ‰æ•ˆèŒƒå›´å†…æ ‡è®°
                    if self.threshold_mode == "binary":
                        thr_vector[idx] = 1.0
                    else:
                        thr_vector[idx] = val  # ä¿ç•™å®é™…é˜ˆå€¼ï¼ˆmAï¼‰

            thr_matrix[n] = thr_vector

        return thr_matrix

    def __load_real_data(self):
        return {}
    
    def __len__(self) -> int:
        return self.num_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬æ•°æ®ï¼ˆè‡ªåŠ¨é€‚é…å¤§æ–‡ä»¶æ‡’åŠ è½½ï¼‰
        
        Args:
            idx (int): æ ·æœ¬ç´¢å¼•ï¼ˆç›¸å¯¹äºå½“å‰æ•°æ®èŒƒå›´çš„ç´¢å¼•ï¼‰
        
        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
                - cmap_data: CMAPå¹…å€¼æ•°æ® (500,)
                - mu_count: è¿åŠ¨å•ä½æ•°é‡æ ‡ç­¾ (æ ‡é‡)
                - threshold_data: è¿åŠ¨å•ä½é˜ˆå€¼æ•°æ® (500,)
        """

        # éªŒè¯ç´¢å¼•èŒƒå›´
        if not (0 <= idx < self.num_samples):
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {self.num_samples})")

        # è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
        actual_idx = self.start_idx + idx

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤§æ–‡ä»¶ï¼ˆh5py.Datasetï¼‰
        if isinstance(self.data_dict["data"], h5py.Dataset):
            # ---- å¤§æ–‡ä»¶æ‡’åŠ è½½æ¨¡å¼ ----
            # ä» HDF5 æ•°æ®ä¸­è¯»å–å•ä¸ªæ ·æœ¬
            data_item = np.array(self.data_dict["data"][actual_idx])  # (500, 2)
            mu_count_val = np.array(self.data_dict["label_num"][actual_idx]).astype(np.float32)
            mu_thr_item = np.array(self.data_dict["muThr"][actual_idx]).astype(np.float32)

            # æå–ç”µæµ (x) ä¸ å¹…å€¼ (y)
            x = data_item[:, 0]
            y = data_item[:, 1]

            # å½’ä¸€åŒ–å¹…å€¼åˆ° [0,1]
            y = (y - y.min()) / (y.max() - y.min() + 1e-8)
            cmap_data = torch.from_numpy(y).float()

            # MUæ•°é‡æ ‡ç­¾
            mu_count = torch.tensor(mu_count_val, dtype=torch.float32)

            # é˜ˆå€¼æ•°æ®ï¼ˆ500ç»´æ˜ å°„ï¼‰
            threshold_data = torch.from_numpy(mu_thr_item).float()

        else:
            # ---- å°æ–‡ä»¶å†…å­˜æ¨¡å¼ ----
            cmap_data = torch.from_numpy(self.cmap_amplitudes[actual_idx, :]).float()
            mu_count = torch.tensor(self.mu_count_labels[actual_idx], dtype=torch.float32)
            threshold_data = torch.from_numpy(self.mu_thresholds[actual_idx, :]).float()

        return cmap_data, mu_count, threshold_data


    @staticmethod
    def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
        """
        æ‰¹å¤„ç†å‡½æ•°ï¼Œå°†å•ä¸ªæ ·æœ¬ç»„åˆæˆæ‰¹æ¬¡æ•°æ®
        
        Args:
            batch: åŒ…å«å¤šä¸ªæ ·æœ¬çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬ä¸º(cmap_data, mu_count, threshold_data)
            
        Returns:
            Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
                - src: {"cmap": tensor} - CMAPæ•°æ®ï¼Œå½¢çŠ¶(batch_size, 500)
                - tgt: {"mus": tensor, "thresholds": tensor} - MUsæ•°é‡å’Œé˜ˆå€¼æ•°æ®
        """
        # è§£åŒ…æ‰¹æ¬¡æ•°æ®
        cmap_data_list, mu_counts_list, threshold_data_list = zip(*batch)
        
        # æ„å»ºsrc: {"cmap": tensor}æ ¼å¼
        src = {"cmap": torch.stack(cmap_data_list, dim=0)}  # (batch_size, 500) - CMAPæ•°æ®
        
        # æ„å»ºtgt: {"mus": tensor, "thresholds": tensor}æ ¼å¼
        tgt = {
            "mus": torch.stack(mu_counts_list, dim=0),           # (batch_size,)
            "thresholds": torch.stack(threshold_data_list, dim=0)  # (batch_size, 500)
        }
        
        return src, tgt

    # =====================================================
    # âœ… é™æ€æ–¹æ³•ï¼šé˜ˆå€¼é‡å¤æ£€æŸ¥ï¼ˆå®Œæ•´é€»è¾‘å°è£…ï¼‰
    # =====================================================
    @staticmethod
    def check_threshold_duplicates(data_path: str, tol: float = 1e-5, verbose: bool = True) -> list:
        """
        æ£€æŸ¥ä»¿çœŸæ•°æ®ä¸­çš„ MU é˜ˆå€¼æ˜¯å¦å­˜åœ¨é‡å¤æˆ–è¿‡è¿‘å€¼ã€‚
        Args:
            data_path (str): .mat æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å« 'label_thr' é”®ï¼‰
            tol (float): åˆ¤æ–­é‡å¤çš„å®¹å·®ï¼Œé»˜è®¤ 1e-5
            verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            list: å«æœ‰é‡å¤é˜ˆå€¼çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        try:
            mat_data = load_mat_data(data_path)
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
            return []

        if "label_thr" not in mat_data:
            print("âŒ æ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'label_thr' é”®ï¼Œæ— æ³•æ£€æµ‹ã€‚")
            return []

        muThr = np.array(mat_data["label_thr"]).squeeze()

        # ç¡®ä¿æ•°æ®ä¸ºäºŒç»´æ ¼å¼è¿›è¡Œå¤„ç†
        if muThr.ndim == 1:
            muThr = muThr.reshape(-1, 1)

        print(f"âœ… åŠ è½½æ•°æ®æˆåŠŸï¼š{muThr.shape[0]} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹æ£€æµ‹é‡å¤é˜ˆå€¼...\n")

        N = muThr.shape[0]
        dup_samples = []

        for n in range(N):
            vals = muThr[n][muThr[n] > 0]
            if len(vals) <= 1:
                continue
            vals_sorted = np.sort(vals)
            diffs = np.diff(vals_sorted)
            if np.any(diffs < tol):
                dup_samples.append(n)
                if verbose:
                    dup_vals = vals_sorted[np.where(diffs < tol)[0]]
                    print(f"âš ï¸ æ ·æœ¬ {n} å­˜åœ¨é‡å¤æˆ–è¿‡è¿‘é˜ˆå€¼: {dup_vals}")

        if verbose:
            if len(dup_samples) == 0:
                print("âœ… æ‰€æœ‰æ ·æœ¬é˜ˆå€¼å‡å”¯ä¸€ï¼Œæ— é‡å¤ã€‚")
            else:
                print(f"\nâš ï¸ å…± {len(dup_samples)} ä¸ªæ ·æœ¬å­˜åœ¨é‡å¤é˜ˆå€¼: {dup_samples}\n")

        print("â€”â€” æ£€æŸ¥å®Œæˆ â€”â€”")
        if len(dup_samples) == 0:
            print("âœ… æ•°æ®é€šè¿‡å®Œæ•´æ€§æ£€æŸ¥ï¼Œå¯å®‰å…¨è¿›å…¥è®­ç»ƒé˜¶æ®µã€‚\n")
        else:
            print("âš ï¸ è¯·æ£€æŸ¥ä¸Šæ–¹è¾“å‡ºï¼Œå»ºè®®æ‰‹åŠ¨æˆ–è„šæœ¬ä¿®å¤é‡å¤é˜ˆå€¼ã€‚\n")

        return dup_samples


if __name__ == "__main__":
    """
    - æ£€æŸ¥ä»¿çœŸæ•°æ®ä¸­çš„è¿åŠ¨å•ä½(MU)é˜ˆå€¼æ˜¯å¦å­˜åœ¨é‡å¤æˆ–è¿‡è¿‘çš„å€¼
    """
    Sim.check_threshold_duplicates(
        data_path="./data/SimDataset/data.mat",  # æ•°æ®è·¯å¾„
        tol=1e-5,                        # å®¹å·®
        verbose=True                     # æ‰“å°è¯¦æƒ…
    )