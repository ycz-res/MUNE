import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score


def get_nonzero_indices(x):
    """
    è·å–éé›¶ç´¢å¼•é›†åˆ
    Args:
        x: (500,) å‘é‡
    Returns:
        set of indices
    """
    return set((x != 0).nonzero(as_tuple=True)[0].tolist())


def mu_threshold_metrics(pred, gt, eps=1e-8, alpha=(1.0, 1.0, 1.0, 0.5)):
    """
    è¯„ä»·æŒ‡æ ‡å‡½æ•°ï¼šåŒæ—¶è¯„ä¼° MU æ•°é‡ã€ä½ç½®å’Œæ•°å€¼ç²¾åº¦ï¼Œå¹¶ç»™å‡ºç»¼åˆåˆ†æ•°
    Args:
        pred: (B, 500) æ¨¡å‹é¢„æµ‹ (0 or é˜ˆå€¼)
        gt:   (B, 500) çœŸå®æ ‡ç­¾ (0 or é˜ˆå€¼)
        alpha: æƒé‡ (count, pos_iou, pos_f1, mae)
    Returns:
        metrics: dict, åŒ…å«å››ä¸ªæŒ‡æ ‡ + ç»¼åˆåˆ†æ•°
    """
    B = pred.size(0)
    acc_count, iou_pos, f1_pos, mae_val = [], [], [], []

    for b in range(B):
        p, g = pred[b], gt[b]

        # --- 1. éé›¶ç´¢å¼•é›†åˆ ---
        set_p = get_nonzero_indices(p)
        set_g = get_nonzero_indices(g)

        # --- 2. MUæ•°é‡å‡†ç¡®ç‡ ---
        acc_count.append(1.0 if len(set_p) == len(set_g) else 0.0)

        # --- 3. ä½ç½® IoU & F1 ---
        inter, union = len(set_p & set_g), len(set_p | set_g)
        iou = inter / (union + eps)
        precision = inter / (len(set_p) + eps)
        recall = inter / (len(set_g) + eps)
        f1 = 2 * precision * recall / (precision + recall + eps)

        iou_pos.append(iou)
        f1_pos.append(f1)

        # --- 4. æ•°å€¼ MAE (ä»…åœ¨ä½ç½®ç›¸åŒå¤„æ¯”è¾ƒ) ---
        common_idx = list(set_p & set_g)
        if len(common_idx) > 0:
            mae = torch.mean(torch.abs(p[common_idx] - g[common_idx])).item()
            mae_val.append(mae)

    # å–å¹³å‡
    acc_count = sum(acc_count) / B
    iou_pos = sum(iou_pos) / B
    f1_pos = sum(f1_pos) / B
    mae_val = sum(mae_val) / max(len(mae_val), 1)

    # ç»¼åˆåˆ†æ•°
    score = (alpha[0] * acc_count +
             alpha[1] * iou_pos +
             alpha[2] * f1_pos -
             alpha[3] * mae_val)

    metrics = {
        "count_acc": acc_count,
        "pos_iou": iou_pos,
        "pos_f1": f1_pos,
        "val_mae": mae_val,
        "score": score  # ç»¼åˆæŒ‡æ ‡ï¼Œç”¨æ¥æŒ‘é€‰æœ€ä¼˜æ¨¡å‹
    }
    return metrics


def comprehensive_metrics(pred, gt, eps=1e-8):
    """
    ç»¼åˆè¯„ä»·æŒ‡æ ‡å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬
    åŒ…å«æ›´å¤šè¯„ä»·æŒ‡æ ‡ï¼Œæ‰¹é‡è®¡ç®—æé«˜æ•ˆç‡
    
    Args:
        pred: (B, 500) æ¨¡å‹é¢„æµ‹
        gt:   (B, 500) çœŸå®æ ‡ç­¾
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
    Returns:
        metrics: dict, åŒ…å«å„ç§è¯„ä»·æŒ‡æ ‡
    """
    B = pred.size(0)
    
    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ©ç 
    pred_mask = (pred != 0).float()
    gt_mask = (gt != 0).float()
    
    # 1. æ•°é‡ç›¸å…³æŒ‡æ ‡
    pred_counts = pred_mask.sum(dim=1)
    gt_counts = gt_mask.sum(dim=1)
    
    count_mae = torch.mean(torch.abs(pred_counts - gt_counts)).item()
    count_rmse = torch.sqrt(torch.mean((pred_counts - gt_counts) ** 2)).item()
    count_accuracy = torch.mean((pred_counts == gt_counts).float()).item()
    
    # 2. ä½ç½®ç›¸å…³æŒ‡æ ‡
    intersection = torch.sum(pred_mask * gt_mask, dim=1)
    union = torch.sum(pred_mask + gt_mask - pred_mask * gt_mask, dim=1)
    iou = intersection / (union + eps)
    mean_iou = torch.mean(iou).item()
    
    # Precision, Recall, F1
    precision = intersection / (pred_mask.sum(dim=1) + eps)
    recall = intersection / (gt_mask.sum(dim=1) + eps)
    f1 = 2 * precision * recall / (precision + recall + eps)
    
    mean_precision = torch.mean(precision).item()
    mean_recall = torch.mean(recall).item()
    mean_f1 = torch.mean(f1).item()
    
    # 3. æ•°å€¼ç›¸å…³æŒ‡æ ‡
    common_mask = pred_mask * gt_mask
    if torch.sum(common_mask) > 0:
        pred_values = pred[common_mask.bool()]
        gt_values = gt[common_mask.bool()]
        value_mae = torch.mean(torch.abs(pred_values - gt_values)).item()
        value_rmse = torch.sqrt(torch.mean((pred_values - gt_values) ** 2)).item()
    else:
        value_mae = 0.0
        value_rmse = 0.0
    
    # 4. ç¨€ç–æ€§æŒ‡æ ‡
    sparsity_pred = torch.mean(pred_mask).item()
    sparsity_gt = torch.mean(gt_mask).item()
    sparsity_error = abs(sparsity_pred - sparsity_gt)
    
    # 5. ç»¼åˆåˆ†æ•°
    # æƒé‡å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´
    composite_score = (
        0.3 * count_accuracy +
        0.3 * mean_iou +
        0.2 * mean_f1 +
        0.1 * (1 - min(count_mae / 10, 1)) +  # å½’ä¸€åŒ–æ•°é‡è¯¯å·®
        0.1 * (1 - min(value_mae, 1))  # å½’ä¸€åŒ–æ•°å€¼è¯¯å·®
    )
    
    metrics = {
        # æ•°é‡æŒ‡æ ‡
        "count_mae": count_mae,
        "count_rmse": count_rmse,
        "count_accuracy": count_accuracy,
        
        # ä½ç½®æŒ‡æ ‡
        "pos_iou": mean_iou,
        "pos_precision": mean_precision,
        "pos_recall": mean_recall,
        "pos_f1": mean_f1,
        
        # æ•°å€¼æŒ‡æ ‡
        "val_mae": value_mae,
        "val_rmse": value_rmse,
        
        # ç¨€ç–æ€§æŒ‡æ ‡
        "sparsity_pred": sparsity_pred,
        "sparsity_gt": sparsity_gt,
        "sparsity_error": sparsity_error,
        
        # ç»¼åˆæŒ‡æ ‡
        "composite_score": composite_score,
        
        # å…¼å®¹æ€§æŒ‡æ ‡ï¼ˆä¿æŒä¸åŸæœ‰ä»£ç å…¼å®¹ï¼‰
        "count_acc": count_accuracy,
        "pos_iou": mean_iou,
        "pos_f1": mean_f1,
        "val_mae": value_mae,
        "score": composite_score
    }
    
    return metrics


def batch_metrics(pred, gt, eps=1e-8):
    """
    æ‰¹é‡è®¡ç®—æŒ‡æ ‡ - é«˜æ•ˆç‰ˆæœ¬
    é€‚ç”¨äºå¤§æ‰¹é‡æ•°æ®
    
    Args:
        pred: (B, 500) æ¨¡å‹é¢„æµ‹
        gt:   (B, 500) çœŸå®æ ‡ç­¾
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
    Returns:
        metrics: dict, åŒ…å«å„ç§è¯„ä»·æŒ‡æ ‡
    """
    return comprehensive_metrics(pred, gt, eps)


def print_metrics_summary(metrics: Dict[str, float], prefix: str = ""):
    """
    æ‰“å°æŒ‡æ ‡æ‘˜è¦
    
    Args:
        metrics: æŒ‡æ ‡å­—å…¸
        prefix: å‰ç¼€å­—ç¬¦ä¸²
    """
    print(f"\n{prefix}è¯„ä»·æŒ‡æ ‡æ‘˜è¦:")
    print("=" * 50)
    
    # æ•°é‡æŒ‡æ ‡
    print("ğŸ“Š æ•°é‡é¢„æµ‹æŒ‡æ ‡:")
    print(f"  MAE: {metrics.get('count_mae', 0):.4f}")
    print(f"  RMSE: {metrics.get('count_rmse', 0):.4f}")
    print(f"  å‡†ç¡®ç‡: {metrics.get('count_accuracy', 0):.4f}")
    
    # ä½ç½®æŒ‡æ ‡
    print("\nğŸ¯ ä½ç½®é¢„æµ‹æŒ‡æ ‡:")
    print(f"  IoU: {metrics.get('pos_iou', 0):.4f}")
    print(f"  Precision: {metrics.get('pos_precision', 0):.4f}")
    print(f"  Recall: {metrics.get('pos_recall', 0):.4f}")
    print(f"  F1: {metrics.get('pos_f1', 0):.4f}")
    
    # æ•°å€¼æŒ‡æ ‡
    print("\nğŸ“ˆ æ•°å€¼é¢„æµ‹æŒ‡æ ‡:")
    print(f"  MAE: {metrics.get('val_mae', 0):.4f}")
    print(f"  RMSE: {metrics.get('val_rmse', 0):.4f}")
    
    # ç»¼åˆæŒ‡æ ‡
    print("\nğŸ† ç»¼åˆæŒ‡æ ‡:")
    print(f"  ç»¼åˆåˆ†æ•°: {metrics.get('composite_score', 0):.4f}")
    print(f"  ç¨€ç–æ€§è¯¯å·®: {metrics.get('sparsity_error', 0):.4f}")
    
    print("=" * 50)
